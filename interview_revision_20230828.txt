from join 
where 
group by having 
select 
order by limit 

with temp as 
(select row_number() over (partition by ltrim(sku,'0'), ltrim(upc,'0') over order by active_flag desc) rnum 
from product.product_set_member),
temp2 as 
(select *, max(rnum) over (partition by ltrim(sku,'0'), ltrim(upc,'0')) mx 
from temp)
select * from temp2 where rnum>1;


with temp as 
(select dense_rank() over (partition by ltrim(sku,'0'), ltrim(upc,'0') over order by active_flag desc) rnk 
from product.product_set_member),
temp2 as 
(select *, max(rnum) over (partition by ltrim(sku,'0'), ltrim(upc,'0')) rnk 
from temp)
select * from temp2 where rnk>1;


with temp as 
(select ltrim(sku,'0') as sku, ltrim(upc,'0') as upc 
from product.product_set_member 
group by ltrim(sku,'0'), ltrim(upc,'0')
--having count(*)>1
having count(distinct uuid)>1
)
select psm.*
from product.product_set_member psm 
join temp on coalesce(ltrim(psm.sku,'0'),'sku')=coalesce(ltrim(temp.sku,'0'),'sku') and coalesce(ltrim(psm.upc,'0'),'upc')=coalesce(ltrim(temp.upc,'0'),'upc');


--concat 
with temp as 
(select ltrim(sku,'0') as sku, ltrim(upc,'0') as upc 
from product.product_set_member 
group by ltrim(sku,'0'), ltrim(upc,'0')
--having count(*)>1
having count(distinct uuid)>1
)
select psm.*
from product.product_set_member psm where coalesce(ltrim(psm.sku,'0'),'sku')||coalesce(ltrim(psm.upc,'0'),'upc') in 
(select coalesce(ltrim(psm.sku,'0'),'sku')||coalesce(ltrim(psm.upc,'0'),'upc') from temp);

col
1
1
2
3
1

col
1
1
1
5

output of inner, left, right, full outer join


nullif(nullif(trim(col),''),'NA') is null 
coalesce(trim(col),'') <> ''
ifnull(col, 2)
case when col1 in (1,2) then 1 
     when (col1=2 and col2=4) or col3=6 then 2 
	 else 0 end as active_flag 
	 
	 
select tbl1.*
from tbl1 
left join tbl2 on tbl1.id=tbl2.id where tbl2.id is null;


select tbl2.*
from tbl2 
left join tbl1 on tbl2.id=tbl1.id and tbl2.name=tbl1.name where tbl1.id is null and tbl1.name is null;


--last_value 
--rank 
select tbl2.*, coalesce(tbl1.id, first_value(tbl2.id) over (partition by name, age), first_value(tbl2.name) over (partition by name, age))
from tbl2 
left join tbl1 on tbl2.id=tbl1.id and tbl2.name=tbl1.name ;


select * from stage
union 
--union all 
select * from tgt 
left join stg on tgt.id=stg.id and tgt.name=stg.name where stg.id is null and stg.name is null;


select coalesce(stage.id, tgt.id),  coalesce(stage.name, tgt.name),
from stage 
full outer join tgt on stage.id=tgt.id and stage.name=tgt.name
where (tgt.id is null and tgt.name is null) or (stg.id is null and stg.name is null);


--full join as inner join 
--insert overwrite into tgt (uuid, id, name)
select coalesce(tgt.uuid, stage.uuid), coalesce(stage.id, tgt.id),  coalesce(stage.name, tgt.name),
from stage 
full outer join tgt on stage.id=tgt.id and stage.name=tgt.name
where (tgt.id is not null and tgt.name is not null) and  (stg.id is not null and stg.name is not null);


select
 *,
 case when tgt.id is null and stg.id is not null then 'res1'
      when stg.id is null and tgt.id is not null then 'res2'
	  else 'res3' end as result
from stage 
full outer join tgt on stage.id=tgt.id and stage.name=tgt.name;


select 
coalesce(stage.col, tgt.col),  
coalesce(stage.col2, tgt.col2),
from stage 
full outer join tgt on stage.id=tgt.id and stage.name=tgt.name;


with temp as 
(select id, cost::int*-1 from tbl)
select *, sum(cost) over (partition by id order by date);

select id, max(marks)
from tbl 
group by id;

select id, max(marks) over (partition by id)
from tbl ;

select id, sum(case when marks=1 then 1 else 0 end) as sm 
from tbl 
group by id;


select tbl1.id, tbl2.name from 
tbl 
cross join tbl2 ;

--inner join with a broader join to behave as cross join 

select tbl1.id, tbl2.name from 
tbl 
inner join tbl2 on tbl.banner=tbl2.banner;

with temp as 
(select country, max(date) as max_date
from tbl
group by country),
temp2 as 
(select country, value, date
from tbl where date<=(select min(max_date) from temp))
select country, sum(value)
from temp2
group by country;


create temporary table tmp
with temp as 
(select uuid, count(distinct uuid) over (partition by sku, upc) cnt from 
product.product_set_member)

delete from product.product_set_member where uuid in (select uuid from tmp);


with temp as 
(select uuid, sku, upc, lead(start_date) over (partition by sku, upc order by start_date asc) ld_start_date from product.product_set_member),
temp2 as 
(
select uuid, dateadd('day', -1 , ld_start_date ) as new_end_date from temp 
)
update price.client_price_denorm cpd 
set end_date=cpd.new_end_date
from temp 
where cpd.uuid=temp.uuid and sku in ('123', '456');



update price.client_price_denorm cpd 
using 
(with temp as 
(select uuid, sku, upc, lead(start_date) over (partition by sku, upc order by start_date asc) ld_start_date from product.product_set_member),
temp2 as 
(
select uuid, dateadd('day', -1 , ld_start_date ) as new_end_date from temp 
)
select * from temp2)
set end_date=cpd.new_end_date
from temp 
where cpd.uuid=temp.uuid and sku in ('123', '456');



select ssh.*, ssp.store_set_parent
from store.store_set_hierarchy ssh 
join store.store_set_hierarcht ssp where ssh.store_set_id=ssp.store_set_parent_id
join store.store_set_hierarcht ssp2 where ssh.store_set_id = ssp2.store_set_id; 


select psm.sku, h.level5 as level1 , h.level 4 as level2, h.level3 as level2, h.level2 as level1 
from psm 
join hiearrchy h on psm.id=h.hierarchy_id;

select e.*
from employee e
join employee m on e.empid=m.mamger_id;


temp.col2 = coalesce(tgt.col5, tgt.col4, tgt.col3, tgt.col2, tgt.col1)

with temp as 
(select max(salary) as mx_salary from tbl)
select tbl1.*
from tbl 
left join tmp on tbl.salary=tmp.mx_salary where tmp.mx_salary is null order by salary limit 1 ;

select * from tbl qualify 
dense_rank() over (partition by emp order by salary)=2 ;

--row to column
with 
tbl as 
(
select id, salary, 
from tbl where company='walmart'
),
tbl2 as 
(
select id, salary
from tbl where company='amazon'
)
select tbl.id, tbl.salary as walmart_salary, tbl2.salary as amazon_salary
from tbl 
join tbl2 on tbl.id = tbl2.id;

%sql
with 
good_data as (
select 1 as id, count(*) as valid_data from dev.event_schema.silver
),
bad_data as
(
  select 1 as id, count(*) as invalid_data from dev.event_schema.error_tbl
)
select g.valid_data, b.invalid_data
from good_data g 
join bad_data b on g.id=b.id;

--column to row 

select id, amazon_salary as salary from temp 
union 
select id, walmart_salary as salary from temp ;


select id, name from tbl 
except 
--minus 
select id, name from tbl2 

select id, name from tbl 
intersect 
select id, name from tbl2 


sku=left(sku, length(sku)-1)
length(col) - length(replace(col,'s',''))
substring(col, 1, 5)
charindex('/', col)
trim(col)
split(col,',')
split(col,'/')[0] / split(col,'/')[1]
split_part(col, ',', 1 )
datediff(minute, current_timestamp, current_temistamp-1)
to_date('12/12/2023','mm/dd/yyyy')
date_format('12/12/2023','mm/dd/yyyy')
explode(split(col,',')) as val
reverse(split(reverse(url),'/')[0])
like '%%'
regexp_like '[A-Z]'
regexp_replace('A-Z', 1)
regexp_replace('A-Z', '') || ' ' || regexp_replace('1-9', '')
concat(regexp_replace('A-Z', '') ,' ' ,regexp_replace('1-9', ''))
soundex(name) == soundex(name2)
jarowinkler_similarity(col1) = jarowinkler_similarity(col2)

select get_json_object(raw_data, '$.name')
select get_json_object(raw_data, '$[0].name')


select to_json(struct(id, name))
from tbl 
group by col;

select collect_list(name)
from tbl 
group by col;

select collect_set(name)
from tbl 
group by col;

select to_json(struct(id, name))
from tbl ;


select to_json(struct(id, name))
from tbl 
group by col;


select to_json(map_from_enteries(struct(id, name)))
from tbl 
group by col;


select to_json(map_from_enteries(array_construct(struct(id, name))))
from tbl 
group by col;

select to_json(map_from_enteries(array_construct(struct(id, name)))) over (partition by col)
from tbl ;

concat_ws(-, col1, col2)

select col, stringagg(id, ', ')
from tbl 
group by col;


select col, listagg(id, ', ')
from tbl 
group by col;

select id, current_date() as date from tbl ;
select len(trim(ltrim(col,0)));


id 
1
2
7
8
9


with temp as 
(select 
id, lag(id) over (order by id) as lg from temp)
select 
id, case when id-1=lg then 0 else ed+1 end as missing_start,
, case when id-1=lg then 0 else lg-1 as end_start,
from temp;


--output of joins 

with temp as 
(select id, split(col,' ') as tweets from tbl )
select tweets, count(*) 
group by tweets; 

--word count 
with temp as 
(select id, split(col,' ') as word from tbl )
select word, count(*) 
group by word; 

--managed tbl 
--by default stored in hive catalog, default database
create table cpd as 
select * from tbl limit 7  ;


--external tbl 
--can plugin external azure container location via sql ui 
create table cpd as 
select * from tbl 
using delta
location '/dbfs//';

describe extended tbl_name;

create table cpd as 
select * from tbl 
using csv
location '/dbfs//'
options (header=True);


create table cpd as 
select * from tbl 
using parquet
location '/dbfs//';


--update, merge, grant, time travel  using delta
--unity catalog allows coomom access control plane, not seperate hive catalog per worspkace


grant select, update on table tbl to user 'test';
revoke select, update on table tbl to user 'test';
grant all previliges on table tbl to user 'test';

--version are nothing but container files 
select * from tbl as of version 1 ; 
select * from tbl as of timestamp '2023-02-02'

truncate table tbl; 
delete from tbl;
delet from tbl where name='test';

alter table tbl_name rename to tbl2;
alter table tbl_name rename column id to id_name; 
alter table tbl_anme add column col5 varchar col7 varcharl; 
alter table tbl_name drop column col5, col7; 

select distinct id, name from tbl ;


--catalogs , schema (database) cannot be created without unity catalog .

create view view_name 
select * from tbl where time_stamp='2023-02-02';

drop view view_name;
drop table tbl;

Merge into tbl using 
(select * from stage) s 
on tbl.id=s.id and tbl.name=s.name 
when matched then update set tbl.col1=s.col1, tbl2.col2=s.col2 
when not matched the insert (tbl.id, tbl.name) values (s.id, s.name);

select id, name , count(*)
from tbl 
where id between 2 and 7 
group by id, name ;


select id, sum(case when tbl.sku is not null and tbl2.marks is not null then 1 else 0 end) as sm 
from tbl 
left join tbl2 on tbl.id=tbl2.id;


-----------------------------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------------------------

l = [1,2,3,4,4,4,5,5,5,5,6,7]

d_start = {}
d_end = {}
for idx, ele in enumerate(l):
  if ele not in d_start.keys():
    d_start[ele] = idx 
  else:
    d_end[ele] = idx 
	
	
n = input()
c = []
for ele in d_start.keys():
  if int(ele) == n :
    c.append(ele)
	
for ele in d_end_keys():
  if int(ele) == n ;
   c.append(ele)
   
print(c)


for key, val in sorted(d.items(), key = lambda x:x[1], reverse=True):
   print(key, val)
    
	
[ key, val for key, val in sorted(d.items(), key = lambda x:x[1], reverse=True) ]


l = [1,2,3,4,3,4]

d={}

for ele in l:
  if ele not in d.keys():
    d[ele]=1
  else:
    d[ele]=d[ele] + 1
	
print(d)


s = 'I am a little man'
d= {'vowels':0, consonants:0}

v = 'aeiou'
for ele in s:
  if ele.strip().lower() in v:
    d['vowels']=d['vowels']+1 
  else:
    d['consonants']=d['consonants']+1
	
print(d)


s = """
col1 col2 col3 
1    2     3
2    2     4
3    2     6
2    2     7
"""
d={}
rows = s.split("\n")
for row in rows:
  if row.strip() ! = '':
     col = row.split(" ")
	 key = col[0]
     val = col[2]
	 if key not in d.keys():
	    d[key]=val 
	 else:
	    d[key]=d[key]+val 
print(d)
  	 
def check_anagaram(s1, s2):
	if sorted(s1)==sorted(s2):
	  return 0 
	return 1 

check_anagaram('I am an apple', 'apple I am ')

print("-"*10)

l = [1,2,3,4,5]
" , ".join(l)


d = {"key 1":2, "key 2":7, "key 3":5, "key 4":9}
s=0
for key, val in d.keys():
  if int(key.split(" ")[1])%2==0:
    s = s + val

print(s)

m = 0 
l = [1,2,3,4]
for ele in l:
 if ele> m:
   m = ele 
   
print(m)



s1 = [1,2,4,5,6]
s2 = [1,2,3,3,4,5]

s1 = set(s1)
s2 = set(s2)
s1.symmetric_difference(s2)
s1.intersect(s2)
s1.union(s2)

s_diff1 = s1-s2     
s_diff2 = s2-s1
s_diff1.union(s_diff2)


pip3 freeze | grep snowflake 
pip3 freeze > requirements.txt

virtualenv venv 
source venv/bin/activate 
vi requirements.txt 
pip3 install requirements.txt 
deactivate 

l1 = [1,2,3,4]
l2=l1 
id(l1)
id(l2)

d1={"key 1":1, "key 2": 4}
from copy import deepcopy
dd = deepcopy(d1)
id(d1)
id(dd)


l1 = [1,2,3,4, 7]
l2=[1,2,3,4,5,6]
c= []
for ele in l1:
 if ele in l2:
  c.append(ele)


l1 = [1,2,3,4, 7]
l2=[1,2,3,4,5,6]
c= []
for ele in l1:
 if ele not in l2:
  c.append(ele)
  
for ele in l2:
 if ele not in l1:
  c.append(ele)  

print(c)

def test_fn(a, b, *l, o=7, **d):
   print(a, b , l, o, d)
   
test_fn(1, 2, 3, 4, o=9, name='test', salary='7')

l1=[1,2,3,4]
l2=["fruits", True, 2, None]
l1.extend(l2)

l1=[1,2,3,4, ["fruits", True, 2, None]]
c=[]
for ele in l1:
  if isinstance(ele, list):
    for ele2 in ele:
	 c.append(ele2)
  else:
    c.append(ele)
	
print(c)

def check_palindrome(s):
  if s==s[::-1]:
    return True 
  else 
    return False 


def check_armstrong(n):
  s = str(n)
  t = 0
  mult = len(s)
  for ele in s:
   t = t + ele**mult
  if t==n:
    return 0 
  else:
    return 1 
	
	
l = [1,2,3,4,5]
partitions=2
c=[]
for x in range(0, len(l),partitions):
  c.append(l[x:x+partitions])
  
s = "I am an apple"
l = s.split(" ")
for x in range(0, len(l), 2):
  print(l[x]+' test')
  
  
  
l = [1,2,3,4,5]
l[2:5]
l[3:5]
s='there is apple'
s[2:4]

l = [1,2,3,4,5,6]
d={}
for x in range(0, len(l), 2 ):
  d[l[x]]=l[x+1]
print(d)
  

for idx, x in enumerate(range(0, len(l))):
  print(f"""element at {idx} is {l[x]}""")
  
  
l1 = [1,2,3,4]
t1 = tuple(l1) # faster and immutable 
l1 = list(t1)
s1 = set(l1)

l1.insert(2, "fruits")
l1.append("apple")
  
  
  
l = [1,2,3,4,5,6,7]
i = iter(l)
next(i)
next(i)

for x in range(0, len(l)):
  next(i)
  
  
def fibonacci_sequence(n):
  a, b = 0, 1 
  for x in range(0, n):
    yield a 
	s = a + b 
	a = b 
	b = s
	
for x in fibonacci_sequence(10):
  print(x)
  
  
def fibonacci_sequence(n):
  if n <=1:
    return n
  else:
    return fibonacci_sequence(n-1) + fibonacci_sequence(n-2)
	
for x in range(0, 10):
  print(fibonacci_sequence(x))

#create dynamic variables
l = ['one', 'two', 'three']
for ele in l:
    locals()['var'+ele] = ele + '_test'
    print(locals()['var'+ele])

l = ['one', 'two', 'three']
d= {}
for ele in l:
    d[ele] = ele + '_test'
print(d)
  
  
s = "apple is here"
v='aeiou'
vowels=0
consonats=0
for ele in s :
  if ele.lower() in v:
    vowels + = 1 
  else:
   consonats += 1
   
print(vowels, consonats)

import logger 
logger.setlevel()
logger.info("INFO: Success")


def HigherOrderFunction(fn):
  def LowerOrderFunction(*args, **kwargs):
     import time 
	 t1 = time()
     res = fn(*args, **kwargs)
	 t1=2 = time()
     print(f"time taken is {t2-t1}")  
     return res	 
  return LowerOrderFunction
  
  
@HigherOrderFunction
def some_function():
  pass
  
  
from datetime import time , timedelta 
dd.strfttime('yyyy-mm-dd')
s.strptime('', '')
dd1 + timedelta('2 days')



class some_class(object):
   def __init__(self, name, age):
     self.name = name 
	 self.age = age 
   
   def inner_function(self):
     print("hello")
	 
   def second_function(self):
     self.inner_function()
	 
if __name__ = '__main__':
  print("execute only if directly executed")
  
  
  
def some_function(n):
  try:
    int(n)
  except Exception as e:
    print(f"some excepion {e}")
	raise("Exception")
  else:
    print("no exception")
  finally:
    print("after executed")
	
	
class parentFunction1(object):
   def __init__(self, name, age):
     self.name = name 
	 self.age = age 
   def work(self):
     print(self.name)
	 
class parentFunction2(object):
   def __init__(self, name, age):
     self.name = name 
	 self.age = age 
   def work(self):
     print(self.name)
  
  
class childFunction(parentFunction1, parentFunction2): 
   def __init__(self, name, age):
     parentFunction1.__init__(self, name, age)
	 parentFunction2.__init__(self, name, age)
	 # super().__init__(self, name, age)
	 
   def work(self):
     print("new def")
	
   def new_job(self):
      print("hello")
   


class Product:
   def__init__(self, name, age):
     self._name = name 
	 self.__age = age
  
  @property  
  def get_age(self):
    return self.__age
  
  @classmethod  
  def set_name(cls, name):
    cls._name=name+'s'
	
  @staticmethod
  def some_function():
    print("some function")
	
	
p = Product('test', 21)	 
p.set_name('test2')
p.get_age
p.somefunction()
Product.set_name('test')	



for key, val in os.environ.items():
  print(key, val)
  
  
os.environ['some_var']='test'


j = '{"id":123, 'tweets':'some#tweet#hash'}'

import json 
d = json.loads(j)
tweets = d['tweets'].split('#')
d={}
for ele in tweets:
  if ele not in d.keys():
    d[ele]=1
  else:
    d[ele]=d[ele]+1
print(d)


d = {"key1":2, "key2":4}
import json 
j = json.dumps(d)
import requests
d = requests.request(method='post', payload=j, header='')
rsponse = json.loads(d)
try:
 if response['status_code']==200:
   print("success")
 else:
  print("fail")
except Exception as e:
  sleep(500)
  d = requests.request(method='put', payload=j, header='')
  
  
d= {'key':1, 'key2':[4,5,6], "key3":{"apple":1, "banana":2}}
d['key2'][1]
d['key3']["banana"] 
  
import threading

t1 = threading.thread()
t2 = threading.thread()
t1.start()
t2.start()
t1.join()

from multiprocessing import process

p1 = process.process()
p2 = process.process()
p1.start()
p2.start()
p1.join()

package with __init__.py
module are python files 
class and def (methods)
from packgage.module import Class

setup.py to builod egg, whl file

type(l1)
type(s1)

with open('dbfs://file', 'r+') as file:
   lines = file.readLines()
   statements = lines.split(" ")
   
   
with open('dbfs://file', 'r+') as file:
   file.write("I am here")
   

import sys 
sys.exit(1)
sys.argv[0]
sys.argv[1]


import os 
os.getcwd()
os.remove(file, recurse=True)
os.makedirs(r'c://test/test2')


c=[]
for dir, subdir, files in os.walk('c://test/test2'):
   for file in files;
     if file.endswith('.txt'):
	 c.append(file)
	   
	   
n = lambda x:x*2
n(2)

l = [1,2,3,4]

list(map(lambda x:x*2, l))


def change_title(s):
  return s.title()

l = ['apple', 'banana', 'orange', 'pear']  
  
list(map(change_title, l))

list(filter(lambda x:x%2==0, l))


def check_int(n):
 if isinstance(n, int):
   return 1
 else:
   return 0 
   
list(filter(check_int, l))

[ele for ele in l if ele%2==0]
{ele for ele in l if ele%2==0}
(ele else 0 for ele in l if ele%2==0)


from package.module.class import function


from dataclasses import dataclass 

class Employee:
  name: str 
  age:int 
  class:int
  city: str 
  
e1 = Employee("test", 2, 3, 'test2')
e2 = Employee("test2", 12, 13, 'test4')


def fn(name: str) -> str:
  return f"hello {name}"

import duckdb as dd
df = pd.read_csv("")  
df1 = dd.query("select * from df").df()
print(df1)

d1 = {1:2, 3:4}
d1 = {1:5, 6:7}
{**d1, *d2}

l1=[1,2,3,4]
l2=[4,5,6,7]
[*l1, *l2]

l1 = [1,2,3,4]
t1 = (4,5,6,7)
list(zip(l1, t1))

s = "Iamehare 2 there"
d=0
c=0
for ele in s:
  if ele.isdigit():
     d +=1
  else:
    c+=1
print(d, c)

list(d.keys())
list(d.values())


for keys in d.keys():
 return keys
 
 
for val in d.values():
 return val
 
 
l = [1,2,3,4, 2]
s=0
for ele in l:
  s +=ele
print(s)

l.max()
l.count(2)
l.sum()
l.min()

# use setup.py to package as EGG or wheel
----------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------



from pyspark.sql import functions as f 
from pyspark.conf import SparkConf

# paramter can be also set in spark_defaults.conf or while spark-submit but overriden by run time parametrers
conf = SparkConf().setAll([('spark.app.name','test'), ('spark.executor.memory','2G'), ('spark.driver.memory','2G'),  ('spark.default.partitions',400), # default partition 200
('spark.master','yarn')])
# can run on local[2] , number of threads in bracket 
conf = spark.conf.set('abfs://storage_account@container.dbfs.core.net', '<sas token>')

spark = SparkSession.builder.config(conf=conf).getOrCreate()
spark.catalog.clearCache()

# all caches are tied to one spark session 

# can also use structTypes
ddl = """
id int, name string 
"""

ddl =  StructType([
StructField("name", stringType, true),
StructField("name", integerType, true)
]
)

ddl =  StructType([
StructField("name", stringType, true),
StructField("nested", StructType([(
      StructField("value1", StringType),
      StructField("value2", StringType)
  ]))
]
)

# path can be local dbfs path or azure path after conf is injected 
# in sql this path has to be configured via query editor
fileDF = spark.read.format("csv").option("inferschema", 'false').option("header", 'true').option("delimiter", "|"). \ 
option("multiline", True).option("path", "<path>").schema(ddl).load()
. 
connection_string = {
'sfhost': '',
'sfport';'',
'sfuser':''
'sfpass':''
}

fileDF = spark.read.format("snowflake").options(**connection_string).load()
fileDF = spark.read.format("parquet").option("path", "<path>").load()
fileDF = spark.read.format("json").option("path", "<path>").load()
fileDF = spark.read.format("avro").option("path", "<path>").load()

kafkadf = spark.readStream.format("kafka").
option("kafka.bootstrap.servers", '<>'). \
option('subscribe', 'invoices'). \ 
option('startingOffets', "earliest"). \
load()


kafkadf = spark.readStream.format("socket").
option("hosts", '<>'). \
option('port', 'invoices'). \ 
load()


l = [1,2,3,4]
sc = spark.sparkContext()
df = sc.paralleize(l)
spark.createDataFrame([('test', 2),('test2', 3)],['name', 'age'])
spark.createDataFrame([()],['name', 'age'])
df2 = df.where("1=2")
pandas_df = df.toPandas() # force collect on driver 
file_df = df.coalesce(1) # force collect on driver
df = pandas_df.toDF() # distributes as spark df 

# dataframe to rdd 
df.rdd

# rdd to dataframe 
df = spark.createDataFrame(rdd, schema)

df.collect() # list of row object 

# one value 

df.first()[1] # first column , start from 0
df.collect()[1][2] # first row, second column 
df.show(truncate=False)
df.display()
display(df)

df.groupBy(f.expr("spark_partition_id()")).agg(f.expr("count(*) as cnt"))

df = df.repartition(10) # can increase or decrease teh number of partitions, results in full shuffle
df = df.coalesce(1) # can decrease the number of partition, lesser network shuffle than repartiton 
df.repartition(100, "id", "name") # make sure the cardinality is not too high , not too low, depends on number of total cores in the machine . 
# too high put high pressure on namenode, and no proper pruning of partition in filters, too low means skewed executors 
df.rdd.getNumPartitions()
# rdd is immutable and so is the dataframe, but every opeartion creates in a new memory space 
# python being a dynmicaaly typed language the same variable starts pointing to the same memory location . 


# creates subfolders, make sure cardinality is not too high or low , for above reason 
df.write.format("csv").mode("overwrite").option("path", "<>").partitionBy("year", "month").save()

# if no proper element of cardinality use bucketing , use hash to put elements in file 
df.write.format("parquet").mode("append").option("path", "<>").bucketBY(100, "id").sortBy("id").save()

# external table : manages metadata but not data, data stored in external locations, used in case of poc or migration project 
# managed table : spark manages data and metadata, data stored in hive catalog (metastore), /dbfs/user/hive/warehouse can override by overridng 
spark.sqk.warehouse.dir in spark_defaults.conf 

df.write.format("delta").mode("overwrite").option("path", "<>").partitionBy("year", "month").saveASTable(<tbl_name>) # delta format supports acid, merge, time travel etc 

# external as path
df.write.format("parquet").mode("append").option("path", "<>").partitionBy("year", "month").saveASTable(<tbl_name>)

# managed as no path
df.write.format("parquet").mode("append").partitionBy("year", "month").saveASTable(<tbl_name>)


df.write.format("jdbc").options(**connection).mode("overwrite").option("path", "<>").partitionBy("year", "month").save()
df.write.format("snowflake").options(**connection).mode("overwrite").option("path", "<>").partitionBy("year", "month").option("lowerBound", 1) \
.option("upperBound", 100000) \
.option("numPartitions", 10).save()

# parquet is the default file format for delta 

# streaming dataframe 

streamingdf = df.writeStream.
format("kafka"). # socket 
outputMode("append"). # complete, update (support aggregation, no aggregation then same as append) , append donot support aggregations
option('kafka.bootstrap.servers','<>').
option('topic', '').
option('checkpointlocation','').
trigger('processingTime = 10 sec'), # 'once', continous 
start()

streamingdf.awaittermination()


# garbage collection in pyspark 
can set via spark.conf.set 
concurrent mark sweep cms :- use parallel thread for ganrbage collection, reduce pause times 
G1GC :- focus on regions of most garbage , reduce pause times
parallel GC :- uses multiple threads for garbage collection, more pause times than GIGC and conncurrent mark sweep.default garbage collector 



--row vs columnn based storage 
when less records read, row is good, lot of records column is good , columns are stored together


OLTP vs OLAP 
olap good for complex queries like snowflake, oltp, transaction based like postresql


clustered vs non clustered index 
clustered , data stored close to each other physically, only one 
non clustered , like a leaf pointing towards data location 



facts and dimension table:- product, store dimension, price, sales facts, snowflake, star schema, star, facts in between and dimensions as star nodes, snowflake, fact in betweem, dimesnion at nodes, but dimesnion linked to other dimesnion tables as primary key (composite key, combination of primary key), foreign key
star denormalized simple
start normalized complex, many joins required

Big data warehouses can result in smaller data marts




denormalization 
one row, table

normalization 
multiple row or table 
1NF :- column contain indivisible values 
2NF :- columns depend on primary key 
3NF :- non key attributes not depend on other non key attributes 



serelization in pyspark 
data is serealzied while storing in RAM, ease of movement across distributed network of computers.
java easiest to use , but not perform best (default)
avro compact binary format, with schema evolution support 
kyro best for large datasets , requires more configuration 



sc = spark.sparkContext()
rdd = sc.textFile('file://')
header = rdd.first()
rdd2 = rdd.filter(lambda x:x!=header)
rdd2.collect() # list of strings 

rdd3 = rdd.flatMap(lambda x:x.split(","))

rdd4 = rdd.map(lambda x:x.split(","))
rdd5 = rdd4.map(lambda x : (x[0], x[2]))
rdd6 = rdd5.filter(lambda x : (x[0]=='apple' & x[2]='banana') |  x[2]='guava')
rdd6.take(10)

rdd7.reduceByKey(lambda x, y : x+y)


df.select(
"id", 
"name", 
"age", 
f.expr("case when id=1 then True else false end as new_colums"), 
f.expr("input_file_name() as file_name"), 
f.expr("row_number() over (partition by id, name order by time_stamp) as rnum"),
f.expr("lead(start_date) over (partition by id, name order by start_date) as ld"),
f.expr("listagg(id, '') over (partition by id, name order by time_stamp) as lagg"),
f.expr("count_approx_distinct(col5) over (partition by id, name) as cnt"),
f.expr("sum(col) over (partition by id, name order by col) as sm"),
f.lit(1).alias("new column"),
(col("marks")+col("average")).alias('new_column'),
f.expr(""""marks + average as marks""")
)
# can use lit as well 
# col not actual column 
# can use f.expr to chain everything and write sql function 

df = df.groupBy("sku", "upc").agg(
f.expr("sum(marks) as sm"),
f.expr("max(marks) as mx"),
f.expr("count_approx_distinct(*) as cnt"),
f.expr("count(*) as cnt"),
)

df = df.groupBy(f.expr("case when id=1 then True else False end")).agg(
f.expr("sum(marks) as sm"),
f.expr("max(marks) as mx")
)

cols = [col for col in df.columns if col.lower().endswith('_sales')]
df.select(*cols)

# no having, so do where after group BY

df = df.where("sm=7")
df.where(" (id=2 and name='test') or age=7 ")



df1 = df.select("id", f.expr("explode(split(tweets,'#')) as val"))
df1.groupBy("tweets").agg(f.expr("count(*) as cnt"))
df.show(7)


# stateteful streaming transformation 
# tumbling window:- fixed window 
# sliding window:- sliding window 
# everything after watermark is rejected and not included in aggregations 

df.withWatemark("timestamp", "1 hour").groupBy("id", Window("time", 10, 5)).agg(f.expr("count(*) as cnt"))

# generate dynamic select

condition = [ f"""nullif(trim({x}),'') as {x}""" for x in df.columns ]
print(condition)
df.selectExpr(*condition)


# genearte dynamic where  

condition=[]
for x in df.columns:
  condition.append(f"{x} is not null")
  
condition = " or ".join(condition)
df.where(condition)


condition = ["""get_json_object(raw_data), $.{x}""" for col in ['col1', 'col2']]
df.select(*condition)

# create a function and return data based on requirement 


# create UDFs

def check_column(col):
   try:
      int(col)
	  if col is not None:
		return 1 
      else:
	    return 0 
   except:
     return 0 
	 
	 
spark.udf.register('check_column', check_column)


df.withColumn("new_column", check_column(col2))
df.select("id", check_column(col2))
	
	
df.withColumn("new_column", f.expr("input_file_name()"))
df.withCoumnRenamed("col", "col2")


df.orderBy("id", f.expr("age desc"))

# sortBy doesnot guarantee order, but faster and not on entire table, order based on partitions 



df2 = kafka_df.select(f.expr("from_json(value, schema) as value"))
df3 = df2.select("value.id", "value.name", f.explode("value.col").alias(newval))
df4 = df3.withColumn("name2", newval.name).withColumn('age2', newval.age).drop(newval)

df.cache()
df.persist()
df.unpersist()


spark.sql("create table test (id, name)")
spark.sql("use database testdb")
spark.sql("grant access on table tbl to user user1")




df.createOrReplaceGlobaltempView("temp_view") # accessible across spark session 
spark.sql("select * from temp_view")

df = spark.table("tbl_name")
df.write.mode("delta").partitionBy("").saveAsTable() # global table , not local 
df.write.mode("parquet").option("path", "<>").load()


df.createOrReplacetempView("temp_view1") 
df.createOrReplacetempView("temp_view2")
spark.sql("""
with temp as (
select * from temp_view t1
join temp2 t2 on t1.id=t2.id 
)
select * from temp""").show(7)


unionDF = df.union(df2)
unionDF.show(truncate=False)

unionAllDF = df.unionAll(df2)
unionAllDF.show(truncate=False)

disDF = df.union(df2).distinct()
disDF.show(truncate=False)


e1 = selectDf.where("nullif(trim(heart_bpm),'') is null").withColumn('error', f.lit('Heart Rate Not received'))
e2 = selectDf.where("nullif(trim(time),'') is null").withColumn('error', f.lit('Time Data Not received'))
e3 = selectDf.where("heart_bpm <= 0").withColumn('error', f.lit('Pulse should be Valid'))
e4 = selectDf.where("lower(trim(model)) = 'undef'").withColumn('error', f.lit('Tracker Not Identified'))
e5 = selectDf.where("lower(trim(kcal)) <= 0").withColumn('error', f.lit('Calories Not Valid'))
errorsDf = e1.unionAll(e2).unionAll(e3).unionAll(e4).distinct()
errorsDf.groupBy("ip", "time", "heart_bpm", "kcal", "version", "model").agg(f.expr("""
                                                                                   concat_ws(',',
                                                                                   collect_list(error) )as error""")
                                                                            ).display()

# streaming df not support except
# selectDf.exceptAll(errorsDf.drop("error")) .display() 
selectDf.where("""
    (nullif(trim(heart_bpm),'') is not null) and
    (nullif(trim(time),'') is not null) and
    (heart_bpm > 0) and 
    (lower(trim(model)) <> 'undef') and 
    (lower(trim(model)) <> 'undef') and 
    (kcal <= 0)
    """) .display()

%sql
%%bash # run bash commands



dbutils.fs.ls('<azure>')
dbutils.widgets.get('<paramter_name>', '<default_value>')
dbutils.secrets.get('<scope_name>', key="my_key")
dbutils.fs.ls('')
dbutils.fs.cp('','')
dbutils.fs.rm('', recurse=True)



df.join(df2, col("id")=col("name"), "inner")
df.join(broadcast(df2), col("id")=col("name"), "inner")

# error class

class Errorvalidation(object):
    def __init__(self, df):
        self.df = df

    def error_validator(self, conditions):
        """make this dynamic/control table driven going forward"""
        # Initialize an empty DataFrame for errors
        errorsDf = self.df.where('1 = 0').withColumn('error', f.lit(''))

        # Create error DataFrames and union them
        for condition in conditions:
            error_df = self.df.where(condition["condition"]).withColumn('error', f.lit(condition["error_message"]))
            errorsDf = errorsDf.unionAll(error_df)

        # Remove duplicates
        errorsDf = errorsDf.distinct()

        # Group and aggregate errors
        agg_expr = f.concat_ws(',', f.collect_list('error')).alias('error')
        result = errorsDf.groupBy(*self.df.columns).agg(agg_expr)
        return result   

%run ../Utilities/errorValidation
# calls the errorValidation module from utilities
errorDf = Errorvalidation(selectDf)
errorDf = errorDf.error_validator([
    {"condition": 'nullif(trim(heart_bpm), "") is null', "error_message": 'Heart Rate Not received'},
    {"condition": 'nullif(trim(time), "") is null', "error_message": 'Time Data Not received'},
    {"condition": 'heart_bpm <= 0', "error_message": 'Pulse should be Valid'},
    {"condition": 'lower(trim(model)) = "undef"', "error_message": 'Tracker Not Identified'},
    {"condition": 'kcal <= 0', "error_message": 'Calories Not Valid'}
]
)


# get a dictionary of values product 
# broadcast variable 
sc=spark.sparkContext
val = sc.broadcast(d)



# accumulator is kind of global counter 
acc = sc.accumlator(0)



# while joining streaming dataframe as well, can add watermark 



# salting earlier for skewness
# when key is skewed

df1 = df.withColumn("salt_key", f.expr("concat(primary_col, rand())"))
df1.repartiton(10, salt_key)


# standalone

spark-submit xyz.jar --spark.driver.memory='2G' --spark.executor.memory='3G'



standalone/local :- in local no cluster manager local[4] (4 threads, * mean all cores in machine), local machine may be or using spark's own cluster manager , driver and worker process in same jvm
can use resource manager .


client vs cluster mode:- client mode like in jupyter notebook, driver on client computer, client is killed, spark dies
cluster mode , driver and worker processes on cluster in background processes , 
program is submitted driver jvm program is created by resource manager in some node, in application master container 
once driver program is created it creates executor container in worker nodes with ram , cpu 

since hadoop is distributed, so spark df i.e rdds are also distributed 



jobs:- number of actions 
stages:- number of wide transfromation (invlove more than 1 row, join, group by, network shuffle), multiple stages talk via stage buffer, can be parallel or sequential 
tasks:- narrow transformation(select, where in parallel)



catayst optimizer applies cost based, rule based optimizations (constant folding, dpp etc )

spark has in memory computation, lower i/o involved in moving data from disk to ram and also partitioned data which makes it fast, but during shuffle/sort , during join, group by, result in network cost in movement across worker nodes.As built on hadoop or ADLS distributed fault tolerant storages, so spark is also fault tolerent.


optimization Techniques 

1> Adaptive query execution :- coalesce skewed partitition, not required salting now , try to change sort merge join to broadcast hash join 
2> Dynamic partition pruning:- triggered by partitionBy, bucketBy , unnecessary partitions are pruned, ignored in run time 
3> Query pushdown:- order of sql query , first on, join, then wide tranformation like group by, then narrow like select, then order limit 
4> cache, persist, repartition:- cache in memory, if reusing multiple time dataframe , or hitting too many actions, but a hint, not a guarantee, 
pushed out based on least recently used , repartitioon is costly, must be on column with medium cardiniality else too many unneccessary partition if cardinality high (so many repeated values) and pressure on namenode or too skewed parition if less cardinality (too few values).
5> spark memory management:- driver memory+pyspark memory , executor memory (reserved memory+tranformation and cache memory (handle the limit well, controlled by parameter else OOM), 
user defined function memory) + pyspark memory (for python workers)
6> JDBC optimizations :- set lower bound upper bound, only one executor make connections, but launch parallel thread  
7> Broadcast variable and joins:- if dataframe/variable si small enough, copy to all executors to avoid network shuffle (efault 10 MB, can be altered, max size 8 GB)
8> spark sepculative execution:- if process is running slow then run on other worker to check if fast 
9> spark dynamic resource allocation:- if very long process, run small processes fast, dynamically allocate resources.



# autoloader automatically process files as they arrive, metadata is taken from checkpoint locatioon, can restart from the point of failure

df = spark.readStream.format("cloudFiles").option("cloudFiles.format", "csv").schema(schema).option("path", "").option("header", True).load()


# rather than spark.conf.set , can also mount azure container using dbutils.fs.mount , can use tenant id, application_id, secret 



# can use existing columns to derive new columns 
df.withColumn('new_column', f.expr("input_file_name()"))
df.withColumn('new_column', f.expr("col1 + col2"))
df.withColumn('new_column', col("col1) + col(col2"))


l = ['col1', 'col2']

df.dropDuplicates(*l)

l = ['col1', 'col2']

df.drop(*l)

df.dropna('any') # drop if any column is null
df.dropna('ALL') # drop if all column is null 
df.na.fill({'gender':'unkown', 'age':0})

df.count()
df.distinct().count()

df.cache()
df  = df.count()

# like most of the pyspark operations which are just dag of operations cache is also lazy , can invoke using count 

# medallion architecture, bronze, silver, gold tables 

# delta tables, can import dlt and annotate spark reads with @dlt.table , can add expectations using @dlt.expect()
automatic worflow creation, live tables , create bronze tables, read from bronze tables in silver tables and thenn in gold tables, functions are used annotated
with @dlt


@dlt.table
@dlt.expect('order_id_not_null', 'order_is is not null')
def bronze_tbl():
  return spark.read.format("csv").option('path','').load()
  
@dlt.table
@dlt.expect('order_id_not_zero', 'order_id<>0')  
def silver_tbl():
 return dlt.read('bronze_tbl').select('order_id', f.expr('col1 + col2' as col))
 
 
def gold_tbl():
  return dlt.read('silver_tbl').select('order_id', f.expr(concat(col1,col2,col3) as col))
  # can add transformations further 
  
  
# for live tables 

import dlt 

@dlt.table
@dlt.expect('order_id_not_null', 'order_is is not null')
def bronze_tbl():
  return spark.readStream.format("cloudFiles").option("cloudFiles.format", "csv").option('path','').load()
  
@dlt.table
@dlt.expect('order_id_not_zero', 'order_id<>0')  
def silver_tbl():
 return dlt.read_stream('bronze_tbl').select('order_id', f.expr('col1 + col2' as col))
 
 
def gold_tbl():
  return dlt.read_stream('silver_tbl').select('order_id', f.expr(concat(col1,col2,col3) as col))
  # can add transformations further 
  
  
%run ../file/

dbutils.fs.run(<notebook>, 60)

Job cluster only one job 
All purpose cluster multiple jobs and interactive queries 
  
  

# https://learn.microsoft.com/en-us/azure/databricks/delta-live-tables/expectations


# schedule jobs, manage alerts via ui, api 
# databricks-cli to manage secret scopes 


# Git integration with databricks repos, azure devops , jenkins 
# https://learn.microsoft.com/en-us/azure/databricks/
# https://learn.microsoft.com/en-us/azure/databricks/visualizations/visualization-types

# https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/

# 1-8 workers, 8 GB of ram, 4 cores (units of parallel execution, also depends how many threads are allowed per core)
# All purpose cluster, interactive work loads and job processing, job cluster for just job trigger






#####notes 



1. Multithreading:


import threading

def calculate_square(number):
    result = number * number
    print(f"Square of {number} is {result}")

if __name__ == "__main__":
    for i in range(1, 11):
        thread = threading.Thread(target=calculate_square, args=(i,))
        thread.start()
In this example, we use multithreading to calculate the squares concurrently. Each number's square is computed in a separate thread.

2. Multiprocessing:


import multiprocessing

def calculate_square(number):
    result = number * number
    print(f"Square of {number} is {result}")

if __name__ == "__main__":
    with multiprocessing.Pool(processes=4) as pool:
        pool.map(calculate_square, range(1, 11))
Here, we use multiprocessing to calculate the squares concurrently. The Pool divides the work among multiple processes.

3. Asynchronous Functions (async/await):


import asyncio

async def calculate_square(number):
    result = number * number
    print(f"Square of {number} is {result}")

async def main():
    tasks = [calculate_square(i) for i in range(1, 11)]
    await asyncio.gather(*tasks)

if __name__ == "__main__":
    asyncio.run(main())
In this example, we use asynchronous functions to calculate the squares concurrently. The asyncio library allows us to write non-blocking code that can handle multiple tasks concurrently without the need for separate processes or threads.

In summary:

Multithreading: Uses multiple threads within a single process to achieve concurrency.
Multiprocessing: Uses multiple processes to achieve parallelism.
Asynchronous Functions: Uses the asyncio library to perform asynchronous operations in a single-threaded event loop, allowing for non-blocking concurrency.
For CPU-bound tasks like the one in this example, multiprocessing is generally more efficient, as it utilizes multiple CPU cores. Async functions are better suited for I/O-bound tasks, where tasks often spend time waiting for external resources (e.g., network requests or file I/O).





*******************************************************extra*******************************************************
*******************************************************************************************************************


with 
pre_final as (

select * , 
case
when cast(split(week,'-')[2] as int ) in (1,2,3) then cast(cast(split(week,'-')[1] as int )-1 as varchar)
else cast(cast(split(week,'-')[1] as int ) as varchar) end as FY 
from combined_category_data
)

select substr(FY,3,2) as FY, week,
CASE
WHEN cast(split(cast(week as varchar),'-')[2] as int ) >= 4 AND cast(split(week,'-')[2] as int ) <= 9 THEN
            'FY' || substr(FY,3,2) || ' 1H'
        ELSE
            'FY' || substr(FY,3,2) || ' 2H'
    END AS  FY_

####################################


col
anil
200
mohan 
400




with temp as 
(select col, row_number() over (order by 1) rnum
from tbl)
select 
case when rnum%2==0 then col else null end as col2,
case when rnum%2%=0 then col else null end as col3,


with temp as 
(select col, row_number() over (order by 1) rnum
from tbl)
select col, lead(col) over (order by rnum) ld
from tbl

col2 col3 
null anil 
200  null 
null mohan 
400  null 

WITH numbered_rows AS (
  SELECT 
    column_value,
    ROW_NUMBER() OVER (ORDER BY some_column) AS row_num
  FROM your_table_name
)

SELECT
  MAX(CASE WHEN row_num % 2 = 1 THEN column_value END) AS col1,
  MAX(CASE WHEN row_num % 2 = 0 THEN column_value END) AS col2
FROM numbered_rows
GROUP BY (row_num - 1) / 2;


with temp as 
(select col, count(distinct uuid) cnt from temp)
select * from temp where cnt>1;

linux 
######


ls -lrt 
ls -lh 
ls /dir/subdir/file.txt
cd /dir/subdir/
cat file.txt | head -n 10 | tail -n 10 | grep "2017" |  grep "2019" | wc -l 
zcat file.gz
grep 
wc -l 
chmod 777 filename.txt
chmod 666 filename.txt
chmod 777 -R AlbertsonsSFExports
azcopy
aws s3 ls 
aws s3 cp
mv /dir/subdir/file.txt /dir2/subdir2/file2.txt
cp /dir/subdir/file.txt /dir2/subdir2/file2.txt
cp -r /dir/subdir/*.txt /dir2/subdir2/.
sudo su e3batch 
su e3batch
free -m 
rm file.txt
rm *.csv
rmdir dir
rm -r dir
rm -rf dir
sftp -i  private_file peng3b01@162.53.99.138
mget filename.txt
mput filename.txt 
pwd
cd ..
vi filename.txt
touch filename.txt
docker exec -it airflow /bin/bash
awk
sed 
grep -e pattern1 -e pattern2 -e pattern3
find -name 'pattern*'
whoami


virtualenv myenv
source myenv/bin/activate
vi requirements.txt
pip install -r requirements.txt
deactivate

zip myfile.zip filename.txt
unzip myfile.zip
history
bash script.sh > wrapper.log
sh script.sh >> wrapper.log
echo > file.txt
echo "hello"
kill -9 <pid>
ctrl + c, ctrl + z, right click to paste
lscpu


rename SME3.PRODUCT.230316210108.csv.gz archive/SME3.PRODUCT.230316210108.csv.gz

%%bash
%%sql

clear 
rm *.csv 

search vi 

esc /
write what to serach 
enter
n for next

esc shift : wq! or q!

insert to change insert/append modes


ssh using putty, moboxterm 
ssinha@canopy.e3internal.com
ssh private key
ssh public key shared with admins generated by putty gen

sh run_price_families.sh price_families lbl
bash run_price_families.sh price_families lbl


# run with & or tmux to run as background process

# try downloading/uploading over network , throughput is of server, we just need to connect to server via our client ssh, databricks etc.


cat wrapper_job_20230418183159.log | grep  "Postgress Record inserted successfully" | wc -l







############################event hub#####################################################################################
Databricks Pool 


https://www.databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html

Databases 

Database :- ACID
A :- atomicity transaction either success or fail
C :- consiteneny 100 debited from debit table then updated to credit table
I :- isolation multiple transaction dont interfere with each other 
D :- durability even after database crash, data is present , written in dask not ram 

limitations 

not high velocity 
not urgent data 
non relational data 
dynamic schema 
can't connect to data source directly, we can't replay data from an offset once machine restarted
pace of generation may not be equal to pace of consumption of data 

Event hub / Kafka is like a streaming database , or a log


Azure Event Hub :- quickly store small events in queue 
Azure Event grid :- event driven architecture, something happen, something has to change 
Azure storage queues :- turn azure blob storage into messaging queue 
Azure Service Bus:- central reliable messaging queue (read command)


Event hub 
event is triggered on a state change, publisher dont care how the event is handled 
performance is essential 

but in message the publisher has an expectation how consumer handles the message or a command 

Decoupling :- storage queues 
Speed/Scalability:- azure event hubs 
reliability :- service bus 

Producer 
Event hubs (serealized) :- consumption doesnot mean event will go off , even if we miss something , we can replay data 
Consumer 


azure event hubs should have at least 2 partitions , consumer connect to specific partition, consumer may not see all event 
offset , how many events consumed, measured in bytes 
not permanent retention 
Data capture :- move to cold storage, like azure blob storage to preserve 

repeatable, temporary stream of events, support many consumers 

Advantages of Event Hub 
#######################
can handle many concurrent connections to event producers
we can increase the number of partitions (converyer belts)
speed of conveyer belt (fast throughput)
can add more consumers (groups)


partition keys :- consumer connected to one partition, but we can divide things in logical order, coloured plates.pensylivania events one partition 
may not be uniformly distributed,m california has less people 


messaging portals :- mailbox , envelope size , mail size


, https (website, generic protocol)  azure event hub dosnot support receiving event from https,
kafka protocol (supported by event hub), AMQP (default) . from programmer persective its same. picture, josn, convert to raw bytes, mail carreer doesnot open 
envelope . 

event properties:- headers


compare to kafka 
################
support low latency 
protocol (mailboxes) kafka protocol supported 
infrastructre (software receive and stores messages, nodes, like post offices) (can be replaced by azure event hubs)

both handle events 
at least one messaging 
partitions (distributed computing)
mesage order guarantee even on replay 



Cluster/Namespace :- Every thing in cluster
Topic (single event strings broken in partitions) called event hubs 
partitions (splits an event in substring,scalability, different nodes supporting different partitions of same event/topic )
offset (numerical identity to identify the partition currently played in the stream of events)
consumer group :- groups treated as one consumer . 


Azure event hub , cant change the number of partitions, we need to define new event hub , kafka can change 
azure rettention period 1 to 7 days , kafka can be modified as we control the infrastucture 
paas (platform as a service), kafka also has managed services like confluent kafka 


Producing events 
Storing Events 
Consuming Events 


subscription > resource group (containers) > event hub namespace (containers) > actual event hub 


namespace (access and cost management)
##############
pricing tier 
througput units 


partitions we need (2-32) : can't change later , depends on how many concurrent readers we expect , each reader connected to a different partition
message retention : how many days,1 for basic tier , enable data capture for long term retention 

download azure cli

azure cli 
#########

azcopy ls "https://prodmerchantdatacatalog.blob.core.windows.net/engage3-prod/Outbound/?sp=racwl&st=2022-10-13T16:17:30Z&se=2023-10-14T00:17:30Z&sip=20.22.237.211&spr=https&sv=2021-06-08&sr=c&sig=R3ib6B38qsvw17ysRM103zRWvD1fcMP%2BVxL7%2BLWTay8%3D" ./


azcopy cp --recursive 'https://prodmerchantdatacatalog.blob.core.windows.net/engage3-prod/Outbound/Sales_2022-10-09.psv?si=engage-3-prod&sip=20.22.248.228&spr=https&sv=2021-06-08&sr=c&sig=0qflyV%2Fz9qZAAzVSXWtkoVic9UnwVpACvGEoGJ9OAoA%3D'
azcopy cp "/home/ssinha/Price_20221004.psv" "https://engage3datapoc.blob.core.windows.net/e3public-dev/bevmo/sales/Price_20221004.psv?sp=racwdli&st=2022-07-11T05:23:28Z&se=2079-12-31T18:30:00Z&spr=https&sv=2021-06-08&sr=c&sig=gl6gJnlMZl53sCpK2EQXuOZeAN%2BPC13o6kLp5zHvsj4%3D"

https://learn.microsoft.com/en-us/cli/azure/eventhubs/eventhub?view=azure-cli-latest
https://learn.microsoft.com/en-us/cli/azure/databricks/workspace?view=azure-cli-latest
https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/manage-resource-groups-cli
https://learn.microsoft.com/en-us/cli/azure/manage-azure-subscriptions-azure-cli
az login 


1> create resource group 


az group create --name cligroup --location eastus 
az group create --name rg_learn --location eastus 

2> creating event hub namespace 

az eventhubs namespace create --name TestCLI --resource-group cligroup -l eastus 
az eventhubs namespace create --name TestCLI7 --resource-group rg_learn -l eastus 

3> create azure event hubs 

az eventhubs eventhub create --name clihub --resource-group cligroup --namespace testCLI 
az eventhubs eventhub create --name clihub7 --resource-group rg_learn --namespace testCLI7 
az eventhubs eventhub create --resource-group rg_learn --namespace-name testCLI7 --name clihub7 --partition-count 4 --retention-time 2 --cleanup-policy Delete
az eventhubs eventhub update --resource-group rg_learn --namespace-name testCLI7 --name clihub7 --partition-count 4
az eventhubs eventhub consumer-group  create --consumer-group-name test-consumer --eventhub-name clihub7 --namespace-name testCLI7 --resource-group rg_learn

az eventhubs eventhub list --resource-group rg_learn --namespace-name testCLI7
az eventhubs eventhub delete --resource-group rg_learn --namespace-name testCLI7 --event-hub-name clihub7 

az group delete --name rg_learn


can create via ui 
#################

search for event hub namespace 
namespace 
standard, subscription, resourcegroup , location, throughput (auto inflate to to automatically scale up and down)
Then create event hub 
name, partition count (1-32). retention , capture (can save data to storage account)

create shared access policy (root, send, listen, send)
get connection string (endpoint url, name of shared access policy, shared access key )

click on listen, copy connection string, primary key, secondary key 

send, listen, manage 
Endpoint=sb://eh-learn-namespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=FhX+G4hvqM3dA5A9ND+PkUEL2wGRv/88i+AEhFKpYaU=



python 

pypi event hub library to read events send with protocol
connection string 
shared access policy 
serealziation method 
checkpoint store (where in stream of events it left off, can use to azure storage or database or file)


https://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-python
https://www.geeksforgeeks.org/asyncio-in-python/



eh-storage

https://learnstorage7.blob.core.windows.net/eh-storage?sp=r&st=2023-09-25T08:46:43Z&se=2023-09-25T16:46:43Z&spr=https&sv=2022-11-02&sr=c&sig=YXlkoUh6jNItaAKcLNK3H4TQevlE1ujBu30nHDNxhu4%3D
https://learnstorage7.blob.core.windows.net/eh-storage?sp=r&st=2023-09-25T08:46:43Z&se=2023-09-25T16:46:43Z&spr=https&sv=2022-11-02&sr=c&sig=YXlkoUh6jNItaAKcLNK3H4TQevlE1ujBu30nHDNxhu4%3D


Consumer groups 

name and offset 
many consumers to consume as fast as event is produced 


checkpoints (If one consumer fails, other picks from offset)
Failover 
Consumer groups

kafka consumer groups different from regular event hub consumer groups 
not viewable in azure portal 
auto creation 
can store offsets 
spans a namespace than attached to one event hub 

can add consumer group in azure portal inside event hubs , just a name 

enable kafka in event hub is just a checkbox comes with standard automatically 

SAS 
OAuth
Enable TLS (SASL_SSL)
Port 9093 

Types of Data 

Metrics: coonections, requests, errors, messages and bytes, data capture to long term storage 
Logs 

Azure Monitor 

got to event hub 
Monitor > Metrics 

Azure Blob Storage (object store, store as binary, for )
Azure Data Lake Storage (ADLS Gen 2) (checkbox hierarchical name space to turn blob storage to adls gen2)

Data capture settings for event hub 

size window when want to capture data, how many mb before batch is triggered 
choose the storage account and container 
time window how many minutes before trigger batch 
(first win whichever occurs first)
Folder Format lots of files, folder format, event hub namespace > datetime or viceversa (stored as avro file, compact, binary format for actual data, json headers for shape of data)
Avro designed for Data serealization to send over wire 
hadoop

max retention time for standard tier is just 7 days 

Blob (binary large object) :  optimized for storing massive amounts of structures of data 


storage account > container > blob 


Block Blob (data stored as blocks, uploads fast)
Append Blob (append only operation like logging, can't delete data, for smaller updates)
Page Blob (read write access, tiny )

Data redundancy (local redunant storage stored within same physical location )
zone storage (different locations copy)
geo storage (asynchrous copy data to a different region)

Azure Data Lake Storage 
Azure Data Lake Analytics (run analytics on large amount of data, like databricks)
adls gen2 has real folders unlike simulated folders in azure blobs
built on blob storage 
hadoop compatible access 
hierarchical namespaces 
cheaper than gen1

create storage account with hierarchical namespace, then container 

event hub > featurs > capture > 2 minutes time window > every 50 mb > donot create empty files > azure storage account > year/month/day 

Streaming Anaytics / Databricks 


Real Time 
Urgent and Expiring (shelf life)
Windows of Time
Live Database (Database not  optimized for speed, can use , but lot of schema needed)
Complex Event Processing

PAAS
Scalable 
Batch/Streaming/CICD/graphs/ML

hadoop make use decreasing hardisk (rom) prices 
spark make use of decreasing ram prices 


# https://github.com/databricks/delta-live-tables-notebooks/tree/main/kafka-dlt-streaminganalytics
# can also connect direct https://www.databricks.com/notebooks/iiot/iiot-end-to-end-part-1.html

databricks creates its own resource group, storage accounts, containers


https://www.databricks.com/blog/2020/12/22/how-to-manage-python-dependencies-in-pyspark.html


databricks 

maven dependency 


https://k21academy.com/microsoft-azure/data-engineer/structured-streaming-with-azure-event-hubs/
download azure cli 


https://learn.microsoft.com/en-us/azure/event-hubs/azure-event-hubs-kafka-overview
https://alexott.blogspot.com/2022/06/delta-live-tables-recipes-consuming.html

# databricks cli

https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/workspace-cli
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/clusters-cli
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/cli/#--set-up-the-cli
https://gfranzini.gitbooks.io/tracer/content/support/command-line-mac-vs.-windows.html
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/clusters-cli


##########################################################################################################################


###################databricks sql interface###############################################################################


/* 
a> subscription / billing and monitoring , create subscription, set budget
b> resource group  rg-dbw-dev-001
c> resource -> databricks (dbw-dev-001) (comes with additional container, secret scope and network watcher)
*/

/*

Unity Catalog

Unity catalog (metastore->catalog->Schema->Tables/Views (user/group management, Metastore, Access Control)) , then compute resources per worspace is seperate 
Before every workspace had hive metastore, access control and compute resources
one unit catalog per region , maintained in adls account 
still legacy hive metatsore is there but they donot have the benefits of unit catalog 



A single place to administer all workspaces 
Grant permission using ANSI sql 
captures audit logs 
captures data lineage 
tag, document and search for data assets 

Data Access control, Data Access Audit, Data Lineage, Data Discovery
premium databricks worspace, workspace and metastore should be in same region, premium blob storage account as root container 


*/


/*
Enabling Unit catalog 
1> Azure storage data storage gen 2 + container (this is storage for unity catalog metastore)
2> Access connector for Azure databricks 
3> Unit catalog enabled databricks workspace 
*/

/*
Data Lake (just raw file structures/unstructured, no schema, no acid etc) + delta enginee (adds capabilities of a warehouse) --> delta lake or data lakehouse 
*/

Set up unity catalog 
--------------------


1> search storage resource (one created with databricks will already be there) create one (premium block blob , locally redudant storage (data replicated in one data center),
enable hierarchial namespace ) Please Note:- Databricks account creates one resource group and storage account.
2> create container within storage account.
3> set up access to databricks to unitycatalog storage account (search access connector for databricks an create)
4> go to storage account created in step 1 and go to IAM , add role assignment storage blob data contributor , managed identity, select members and 
select access connector for databricks 
5> In iam role assignment can see assignment 
6> create mestrore go to databricks , manage account , data tab , create metastore , select correct region , give adlks gen2 storage@container name ,
copy resource id from access connector to databricks 
7> In databricks add workspaces , one in same region can be selected , pricing tier should be premium 
go to worspaces and can open , click and can see configurations and manage permissions , can see workspaces in metastore and delete , add users and 
manage settings 
(no access with azure created with epam id)


--can create other external locations and programatically pass  (can use database in place of schema)

create catalog catalog_name managed location url;
create schema catalog_name.schema_name managed location 'abfss://container@storage_account.dfs.core.windows.net/';
describe schema extended schema_name;
create table catalog.schema.table(col1 string);
describe table extended catalog.schema.table;


(old method was to access an external location using sas token in spark.conf.set)

create external location
------------------------

(steps as above but different )
In databricks we have catalog, external data, storage credentials , copy and past connector id , now click on external locations 
create location provide container@storage url, storage-credential , test connection
can use it to create external tables, use external locations 

Now create external location

new acces connector for databricks 
create storage account with hierarchical namespace premium, add a container 
give 1 storage blob data contributor access to the storage container 

catalog > external data > storage credential > give name ext-storage > copy resource id of access connector for databricks >
external location 

abfss://container02@storagee01.dfs.core.windows.net/

/subscriptions/48c839e2-ee7a-40b1-8fd5-812ea7bf764c/resourceGroups/resourcegroup01/providers/Microsoft.Databricks/accessConnectors/accessConnector02

can give access etc now

external table 


-- creating an external table USING a csv data file and specifying the header option as True (please replace the LOCATION with your own)
create table
station_data_csv
(station_id STRING, station_name STRING)
USING csv
LOCATION 'abfss://citibike-ext@extstorage639.dfs.core.windows.net/station_data.csv' --please update the LOCATION path for your specific location
OPTIONS (header=True);



https://learn.microsoft.com/en-us/azure/databricks

https://azure.microsoft.com/en-gb/pricing/calculator/


Summary of unity catalog
######################################################################################################################


https://techcommunity.microsoft.com/t5/fasttrack-for-azure/working-with-unity-catalog-in-azure-databricks/ba-p/3693781


1> managed and external table can be created in both hive catalog (default database) and unity catalog . Earlier we used to create manage table 
normally and for external table we would inject spark.conf.set('<container>','<sas token>') and create table by giving path programmatically or in SQL 
2> create access connector for azure databricks 
3> created a premium block blob storage with hierarchical namespace 
(databricks also creates it own storage ignore to hold hive catalog info, dbfs etc)
4> create container 
5> assign Storage Blob Data Contributor role in IAM , managed identity , select members and use access connector for databricks created 
copy Resource ID
/subscriptions/48c839e2-ee7a-40b1-8fd5-812ea7bf764c/resourceGroups/resourcegroup01/providers/Microsoft.Databricks/accessConnectors/access-coonector01
6> Go to manage account , data, create metastore 
unitycatalogmetadata@storagee001.dfs.core.windows.net
assign to workspaces , worskapce is actually the databricks instance name databricks01
congrats message will come (Enable Delta Sharing to allow a Databricks user to share data outside their organization, but hold on to this ), can edit , in workspaces can see all databricks instance in same region 
in the user management can add users 
create cluster and start 

In the SQL editor can see the new catalogs, database(schema), tables created 


%sql
drop catalog IF EXISTS new_managed_catalog cascade;


Let’s create our first catalog, and managed table.

spark.sql('''create catalog if not exists myfirstcatalog ''') (by default creates information_schema database and default database)

spark.sql('''create database if not exists myfirstcatalog.mytestDB''')

#read the sample data into dataframe

df_flight_data = spark.read.csv("/databricks-datasets/flights/departuredelays.csv", header=True)

df_flight_data.createOrReplaceTempView("temp_tbl")

 

%sql

create table if not exists myfirstcatalog.mytestDB.myFirstManagedTable

AS

Select * from temp_tbl


spark.sql("""create table myfirstcatalog.mytestDB.myfirsTBL(id int, name string)""")


%sql
insert into myfirstcatalog.mytestDB.myfirsTBL
select 1 , 'apple' union all 
select 2 , 'mango';

%sql
describe extended myfirstcatalog.mytestDB.myfirsTBL;

Type MANAGED
Location abfss://unitycatalogmetadata@storagee001.dfs.core.windows.net/568af27d-9634-42c9-9efc-d76d8a0d86f9/tables/1b549004-6d80-47df-8b04-68ca14252fdc
Provider delta
Owner somu.sinha727@outlook.com

drop table drops both data and metadata no immediately but in 30 days 


create table myfirstcatalog.mytestdb.trips_ext if not exists 
select as from samples.nyctaxi.trips 
location abfss://ext-storage@storagee001.dfs.core.windows.net/mytestdb/trips_ext;


Now creating external table 

catalog > data > external data 
create external storage and location 
abfss://ext-storage@storagee001.dfs.core.windows.net/
grant permissions
test connection 

%sql 
describe catalog samples;
use catalog samples;
show databases;
use database nyctaxi;
show tables;
select * from trips;

spark.sql('''create catalog if not exists myfirstcatalog ''')
spark.sql('''create database if not exists myfirstcatalog.mytestDB''')


%sql
create table if not exists  myfirstcatalog.mytestdb.trips_ext
USING DELTA 
location 'abfss://ext-storage@storagee001.dfs.core.windows.net/mytestdb/trips_ext'
select * from samples.nyctaxi.trips 

%sql 
select * from myfirstcatalog.mytestdb.trips_ext;

%sql 
describe extended myfirstcatalog.mytestdb.trips_ext;


%sql 
drop table myfirstcatalog.mytestdb.trips_ext;

# metadata dropped but data not dropped from storage 

#read the sample data into dataframe

df_flight_data = spark.read.csv("/databricks-datasets/flights/departuredelays.csv", header=True)

#create the delta table to the mount point that we have created earlier

dbutils.fs.rm("abfss://dbkdata@adldbkunityctlg.dfs.core.windows.net/mytestDB/MyFirstExternalTable", recurse=True)
df_flight_data.write.format("delta").mode("overwrite").save("abfss://dbkdata@adldbkunityctlg.dfs.core.windows.net/mytestDB/MyFirstExternalTable")
# can also use option("path", "") # presence of path qualifies it as an external table 

%sql
create table if not exists myfirstcatalog.mytestDB.MyFirstExternalTable
USING DELTA
LOCATION 'abfss://dbkdata@adldbkunityctlg.dfs.core.windows.net/mytestDB/MyFirstExternalTable'


# For Streaming

(device_stream.writeStream.format("delta")
        .outputMode("complete").option("checkpointLocation", "/dbfs/delta/events/_checkpoints/gold/device_stream").start("abfss://gold@storagee001.dfs.core.windows.net/device_stream/"))

spark.sql("""
create table if not exists dev.event_schema.gold_device_stream
USING DELTA
LOCATION 'abfss://gold@storagee001.dfs.core.windows.net/device_stream/'
""")

%sql
select count(*) from dev.event_schema.gold_device_stream;

(bpm_stream.writeStream.format("delta")
        .outputMode("complete").option("checkpointLocation", "/dbfs/delta/events/_checkpoints/gold/bpm_stream").start("abfss://gold@storagee001.dfs.core.windows.net/bpm_stream/"))

spark.sql("""
create table if not exists dev.event_schema.gold_bpm_stream
USING DELTA
LOCATION 'abfss://gold@storagee001.dfs.core.windows.net/bpm_stream/'
""")

%sql
select count(*) from dev.event_schema.gold_bpm_stream;

# grant access
%sql
GRANT SELECT ON dev.event_schema.bronze TO `somu.sinha727@outlook.com`;
GRANT USE SCHEMA, CREATE TABLE ON SCHEMA dev.event_schema TO `somu.sinha727@outlook.com`;


create table
station_data_csv
(station_id STRING, station_name STRING)
USING csv
LOCATION 'abfss://citibike-ext@extstorage639.dfs.core.windows.net/station_data.csv' --please update the LOCATION path for your specific location
OPTIONS (header=True);

Type EXTERNAL
Location abfss://ext-storage@storagee001.dfs.core.windows.net/mytestdb/trips_ext
Provider delta
Owner somu.sinha727@outlook.com

Add more external locations 

create containers bronze, silver, gold as external sources and tables under same storage in azure containers, enabel storage and location in external data, then abstract them as table as above
write data see the file refresh in container and table is an abstraction of that file
create storage, location under catalog , external data 
abfss://silver@storagee001.dfs.core.windows.net/ , test connection
checkpoint under local dbfs storage /dbfs/delta/events/_checkpoints/ (can create checkpoint directory in azure too) , test connection 
For grant access look at the document (to same asset , multiple databricks instances i.e workspaces can be given access)
dbutils.fs.rm("abfss://error@storagee001.dfs.core.windows.net/", recurse=True)
can create reports out of these tables now.
##########################################prod setup#################################################################

%sql
create catalog dev;
######################################################################################################################

SQL warehouses create , like cluster , choose serverless


select tpep_pickup_datetime,tpep_dropoff_datetime,trip_distance,fare_amount,pickup_zip,dropoff_zip
from samples.nyctaxi.trips;


select tpep_pickup_datetime,tpep_dropoff_datetime
from samples.nyctaxi.trips;

go inside workspace, create folder and add query from there 

queries refrence

catalog.schema.table 
catalog.database.table 

use catalog samples;
use database nyctaxi;
select * from trips;


database and schema are same in databricks unlike snowflake 


need unity catalog to create catalog , create schema/database, create table etc , can also do from UI data tab
otherwise by default can create hive_metastore catalog under default schema (database) stored in a configurable path /user/hive/warehouse 
controlled by spark.sql.warehouse.dir 

before unity catalog hive catalog metstore existed for each workspace, 	seperate access for workspaces and no synchronization between workspaces. 
user, access management , grant permissions, single metstore , user audit logs , data assets lineage (created), tag and search data assets for multiple workspaces.any changes to security policies can be propagated.
each worskapce still has hive mesttore for backward compatibility.

requirements 
##############
premium databricks worskpace
workspace and metastore should be in same region
premium blob storage account as root container 


Managed Table 

Manages metadata and underlying data file
When you drop a managed table the metadata and underlying data is deleted (by default stored in hive_metastore catalog (/dbfs/user/hive/warehouse/) under default database ) controlled by spark.sql.warehouse.dir 
If we have permission to create unit catalog metastore as above, it will be created in unit catalog metastore.
Managed Tables are the default when creating tables. 
Delta Lake format (built on top apache parquet file format) table data as parquet files 
delta logs created has benefits of delta table acid 
hive metastore is also a catalog, has databases and tables within, table created within are still in delta format, can also create unity catalog , with storage container in azure, then tables within
stored as parquet files in azure 
hive metastore actually is also stored in azure container than got created automatically with databricks account in root folder, may not have permission 
to view it.
Dropping a managed in databricks sodt deletes the data, then deletes after 30 days.


External Table

Manage metadata only, structure on top of data stored somewhere else 
Used when migration projects 
Droping a table only drops the metadata and underlying data is unaffected 
Must specify location while creating an extrernal table 

Delta lake format, Parquet Format, orc format, csv format, json format, txt format , avro format 

Managed external is independent of where we want to create the table , in hive metastore catalog or unity catalog.


Course+Resources+-+SQL+Code.zip
https://learn.microsoft.com/en-us/azure/databricks/lakehouse/medallion

Bronze :- any file format 
Silver :- Delta 
Gold :- Delta 


parquet file is columnar file format 


when query is first run, its cached in memory or in disk.


https://learn.microsoft.com/en-us/azure/databricks/sql/user/alerts/
https://learn.microsoft.com/en-us/azure/databricks/sql/admin/query-caching
https://learn.microsoft.com/en-us/azure/databricks/sql/admin/query-profile
https://learn.microsoft.com/en-us/azure/databricks/sql/admin/query-history

delta format is parquet files with additional acid and data governance properties 



https://learn.microsoft.com/en-us/azure/databricks/sql/user/queries/query-parameters
https://learn.microsoft.com/en-us/azure/databricks/sql/user/dashboards/
# create graphs for each query editor with + symbol
# then add to dashboards

# can add user in azure 
# resource group, IAM , add role assignment , Reader , Review and Assign , add user to databricks account as well manage account , add user , can add group
and add user to group , go to worskpace and add permissions by adding new user or group 

can manage user via admin settings and grant permissions 
admin console groups entitelements 

got to data and access metastores 

access needs to be granted on object basis for users workspaces, clusters, warehouses etc  


https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/sql-endpoint-acl
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/workspace-acl#folder-permissions
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/query-acl
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/dashboard-acl
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/

https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/manage-privileges/privileges
https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/security-grant
https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/security-revoke
https://learn.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/is_account_group_member
https://learn.microsoft.com/en-us/azure/databricks/ingestion/copy-into/tutorial-notebook
https://learn.microsoft.com/en-us/azure/databricks/data-sharing/
https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/jobs-2.0-api

grant use schema to user <>;

create or replace view course_project.citibike.vw_pii_demo
select 
case when is_account_group_member('test_group') then 'REDACTED'
     else ride_id end as ride_id
from tbl ;


select * from course_project.information_schema.tables;
select * from system.information_schema.tables;

see lineage graph 


https://www.youtube.com/watch?v=GN6ICac3OXY
https://learn.microsoft.com/en-us/azure/databricks
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/workspace-acl
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/cluster-acl
https://learn.microsoft.com/en-us/azure/databricks/security/auth-authz/access-control/
https://learn.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes
https://gfranzini.gitbooks.io/tracer/content/support/command-line-mac-vs.-windows.html
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/#--set-up-the-cli
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/clusters-cli
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/workspace-cli
https://docs.databricks.com/api/azure/workspace/clusters
https://learn.microsoft.com/en-us/azure/databricks/archive/dev-tools/cli/
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/tutorial
https://stackoverflow.com/questions/36345136/use-unix-based-commands-with-anaconda-in-windows-operating-system
https://code.visualstudio.com/docs/python/python-tutorial#_run-hello-world
C:\Users\Somu_Sinha\PycharmProjects\
C:\Users\Somu_Sinha\source\repos
# https://github.com/databricks/delta-live-tables-notebooks/tree/main/kafka-dlt-streaminganalytics
# can also connect direct https://www.databricks.com/notebooks/iiot/iiot-end-to-end-part-1.html



KA230912V5295709




26.27 oct - 5 nov
14, 15 nov -  21 nov 


https://learn.microsoft.com/en-us/azure/databricks/

C:\Users\Somu_Sinha\test.py


--connect python in vscode
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/tutorial
https://learn.microsoft.com/en-us/azure/databricks/dev-tools/vscode-ext/dev-tasks/
https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/compute
https://code.visualstudio.com/docs/python/python-tutorial#_run-hello-world
https://datasavvy.me/2022/11/23/creating-a-unity-catalog-in-azure-databric
https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/enable-workspaces
https://community.databricks.com/t5/data-governance/step-by-step-process-to-create-unity-catalog-in-azure-databricks/td-p/6562/page/2

--connect databricks to visual studio code 
https://www.youtube.com/watch?v=Quh1TuJQurA
https://www.youtube.com/watch?v=tThmRuyp2cA
https://www.youtube.com/watch?v=M2Et5aBj2aw

subscription > resource group > databricks|adls gen2 etc 


https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/account
https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/




https://learn.microsoft.com/en-us/azure/databricks/data-governance/unity-catalog/get-started#requirements


Visual Studio Professional Subscription
     

#############
Documentation
##############
https://learn.microsoft.com/en-us/azure/databricks/
https://spark.apache.org/docs/3.1.1/api/python/reference/pyspark.html
https://docs.python.org/3.11/tutorial/index.html
https://docs.snowflake.com/en/sql-reference
https://git-scm.com/docs

###########################################################################################################################


Softwares
#########
Pycharm
Dbeaver
Mobaxterm
Anaconda (Jupyter)
VSCode Professional
Cygwin
GitBash
Azure CLI

#######################################################Db Connect VS Code################################################

GIT repo in web UI 
and User Settings Linked Accounts, sign in 
need to give access in github applications too 
https://github.com/settings/installations/42236144
create feature branch 
create pull to avoid merge conflicts 
can use git notebooks to run jobs as well
https://learn.microsoft.com/en-us/azure/databricks/repos/
https://www.youtube.com/watch?v=k9Kuz19ByNg
https://learn.microsoft.com/en-us/azure/devops/repos/?view=azure-devops
https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html

CICD 


Event Hub/kafka is like a funnel , when we try to put oil in a jar, without it gets scattered
can also replay, live database
speed of generation may not be equal to speed of consumtipn, can manage throughput 



https://adb-2066419697562895.15.azuredatabricks.net/?o=2066419697562895#notebook/1360466229820364/dashboard/4384162943802101
https://adb-2066419697562895.15.azuredatabricks.net/?o=2066419697562895#notebook/1360466229820364/dashboard/4384162943802101/present


somu.sinha727@outlook.com
Golu123@


Azure event hub Partitions are a data organization mechanism that relates to the downstream parallelism required in consuming applications. 
The number of partitions in an event hub directly 
relates to the number of concurrent readers you expect to have. Learn more about partitions.


https://learn.microsoft.com/en-us/answers/questions/785512/is-databricks-available-in-free-tier-and-how-can-i



DbConnect/Vs Code Integration
##############################

Extensions > Databricks > Databricks SQL 
copy http path from warehouse in databricks sql 
daatbricks symbol left , configure via 
gear icon, type database url from chrome

https://adb-604768739209769.9.azuredatabricks.net/
set .databrickscfg

[<some-unique-profile-name>]
host = https://adb-2066419697562895.15.azuredatabricks.net/
token = dapibcc8dc2b2950c9ee564927238219e95d-3
cluster_id = 0922-084852-v9x2rot4

user settings> developer > generate access token dapi6218cf408e3f3621d194afd7c6d60afb-3

admin settings to give access to users 

got to clusters and copy cluster id 0928-081253-okm4hk59

similarly set .databricks-connect 

view > configure autocomplete for databricks global 

use azure cli to login

unity catalog must be enabled 

install required python version 


Python: Select Interpreter in view > command palette 

pip install 'databricks-connect==14.0.0' 

edit 
%USERPROFILE%\.databrickscfg

[default]
host = https://adb-604768739209769.9.azuredatabricks.net/
token = dapi6218cf408e3f3621d194afd7c6d60afb-3
cluster_id = 0928-081253-okm4hk59


%USERPROFILE%\.databricks-connect
{
  "host": "https://adb-604768739209769.9.azuredatabricks.net/",
  "token": "dapi6218cf408e3f3621d194afd7c6d60afb-3",
  "cluster_id": "0928-081253-okm4hk59",
  "org_id": "0",
  "port": "15001"
}

use it to autorize databricks instance and cluster 

create vnenv command pallete , use requirements.txt

python3 -m venv .venv



Pycharm 

edit 
%USERPROFILE%\.databrickscfg

[default]
host = https://adb-604768739209769.9.azuredatabricks.net/
token = dapi6218cf408e3f3621d194afd7c6d60afb-3
cluster_id = 0928-081253-okm4hk59


%USERPROFILE%\.databricks-connect
{
  "host": "https://adb-604768739209769.9.azuredatabricks.net/",
  "token": "dapi6218cf408e3f3621d194afd7c6d60afb-3",
  "cluster_id": "0928-081253-okm4hk59",
  "org_id": "0",
  "port": "15001"
}


View > Tool Windows > Python Packages
check version (should sync with databricks version)
databricks-connect PyPI repository


https://docs.databricks.com/en/dev-tools/databricks-connect.html

# test code 

from pyspark.conf import SparkConf
from databricks.connect import DatabricksSession
from databricks.sdk.core import Config
config = Config(profile="default", cluster_id="0928-081253-okm4hk59")
conf = SparkConf().setAll([('spark.app.name', 'test-app')])
spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()
df = spark.createDataFrame([('apple','2'), ('banana','3')],['col1','col2'])
df.show()



from databricks.connect import DatabricksSession
from databricks.sdk.core import Config

def DatabricksConnect():
    config = Config(profile="default", cluster_id="0928-081253-okm4hk59")
    spark = DatabricksSession.builder.sdkConfig(config).getOrCreate()
    return spark
	
	
conda create -n test_env python=3.6.3 anaconda
conda activate test_env
pip install -r requirements.txt
conda deactivate
conda remove -n test_env --all

pip freeze > requirements.txt



VS Code 
#######

Unit catalog must be Set 
open vs code from anaconda 

%USERPROFILE%\.databrickscfg

[default]
host = https://adb-604768739209769.9.azuredatabricks.net/
token = dapi6218cf408e3f3621d194afd7c6d60afb-3
cluster_id = 0928-081253-okm4hk59


%USERPROFILE%\.databricks-connect
{
  "host": "https://adb-604768739209769.9.azuredatabricks.net/",
  "token": "dapi6218cf408e3f3621d194afd7c6d60afb-3",
  "cluster_id": "0928-081253-okm4hk59",
  "org_id": "0",
  "port": "15001"
}

In anaconda 
python3 download python 
conda in admin mode 
conda install m2-base
conda create -n pocenv
conda activate pocenv
pip install -r requirements.txt
conda deactivate
conda remove -n pocenv --all

pip freeze > requirements.txt

pip install azure-eventhub==5.2.0
pip install azure-storage-blob==12.6.0
pip install avro-python3==1.10.1
pip install Faker==19.6.2
pip install colorama==0.4.6
pip install databricks-connect==14.0.0
pip install databricks-sdk==0.9.0
conda install ipykernel
conda install jupyter

pip uninstall databricks-connect
pip uninstall databricks-sdk

pip uninstall azure-eventhub==5.2.0

In Vs Code 



Python: Select Interpreter , choose conda env 

Databricks 14.0 (includes Apache Spark 3.5.0, Scala 2.12)
conda info --envs 

ctrl+c to stop code 


In terminals use cmd and python 


add this to run code from other packages 


import sys
sys.path.append("../StreamingAPP")

view command palette databricks autocomplete


for ipynb file, select the same conda interpreter 

unistall pylance from extensions to autocomplete to work

Upload and run file on databricks 
run file on databricks as workflows 


https://docs.databricks.com/en/notebooks/best-practices.html


#############################################Git Processes##############################################################

somusinha3@penguin:~$ git --version
git version 2.30.2
somusinha3@penguin:~$ ssh-keygen -t rsa -b 4096 -C "somu.sinha3@gmail.com"
Generating public/private rsa key pair.
Enter file in which to save the key (/home/somusinha3/.ssh/id_rsa):
Created directory '/home/somusinha3/.ssh'.
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/somusinha3/.ssh/id_rsa
Your public key has been saved in /home/somusinha3/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:e28KB07qr/PdpVmN7nt8dGBGTEQjgthZBxn5bDxJKCc somu.sinha3@gmail.com
The key's randomart image is:
+---[RSA 4096]----+
|       o +=*o==  |
|      . E =o..o. |
|         + = o   |
|            B +  |
|        S  . + . |
|       + o     oo|
|      . + o   +oo|
|     ..  = o.*  +|
|      o=o oo=.+o.|
+----[SHA256]-----+
somusinha3@penguin:~$ cd /home/somusinha3/.ssh
somusinha3@penguin:~/.ssh$ ls
id_rsa  id_rsa.pub
somusinha3@penguin:~/.ssh$ cat id_rsa.pub
chmod 600 id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDZ5znmMsWXEfMeYmfO8YeeUhoF3ELvOBvopUhCinywVdVaKkCIfMImYCAcTBVo2Wg1DxKGDphENhThASyIq2t8/JAzFJIkxvJYGUE1ysDc1JXtR1phINIh05YI5tKIAPZ+/3o8tnNm+Sj5EX5A+03uCuHKlahovoAKQZtKXvR3NTjTKTgdwzG2DdYTAiJbdzSL7KLLn86b2Fbxubfg4G+U6wvmhBhtNJxjeZuqxBDO3UZwQ0N8prLxUm1NxO6yIea9IVuTHaaBI26CY5ksFIxQud3XVy933gYKfJraBNSshwQmPfOP4lLxMFVvg3OrIkVhozH/GC6tGXc2NT9tJxUHWyVrjFU8PR+iKIlfnqec0y6TLt7L7/8t27eHow8qlo5U6gT7PAeUkt9K0VjirWxmj1Ec8UNca4GXeSlisb3IH3ZYRpYFxK/ajMHsES4dD0BlAGsQd4ZS8hub4SkvM8H/mqUvwvLkJRLxYC/mHJ2ZOcNW4RyLYtdvTNvMlKvW1q3VEn56pXWW/dxdUNaTVR5Pg+3hDbN2rWzdF+DQp2MpTAhDj3NbX5VPYmJEyYaXOvJpZbYRJfqRqAtBC4yLIubpQMq8FEF9dvj9NbuDQEHTUrzuqskp6zTxO8WFYNHgMkomD3LNF5jD7k+D2hkkKF5hk5g5Pz3aD53NbfNKev5bVQ== somu.sinha3@gmail.com
somusinha3@penguin:~/.ssh$ eval "$(ssh-agent -s)"
Agent pid 818
cd git_pull
somusinha3@penguin:~/.ssh$ ssh-add -K /home/somusinha3/.ssh/id_rsa
Enter PIN for authenticator:
Provider "internal" returned failure -1
Unable to load resident keys: invalid format
somusinha3@penguin:~/.ssh$ ssh-add /home/somusinha3/.ssh/id_rsa
Identity added: /home/somusinha3/.ssh/id_rsa (somu.sinha3@gmail.com)



windows

ssh-keygen -t rsa -b 4096 -C "somu.sinha@engage3.com"
eval "$(ssh-agent -s)"
ssh-add /c/Users/AL2485/.ssh/id_rsa

# https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent



###########

git checkout -b master
git push -u origin  master
git checkout -b qa
git push -u origin  qa
git checkout -b dev
git push -u origin dev
eval "$(ssh-agent -s)"
ssh-add /home/e3custdataload/.ssh/git_private
Identity added: /home/e3custdataload/.ssh/git_private (/home/e3custdataload/.ssh/git_private)


cp -r lighthouse_link_extracts /local/home/e3custdataload/docker-airflow/dags/2022-03-23/engage3-etl-implementations.lighthouse_exports_automation/lighthouse_link_extracts

rm -rf git_pushes_2022_03_23




git checkout -b master
git add .
git commit -m "Lighthouse codes"
git push -u origin  master
git checkout -b qa
git push -u origin  qa
git checkout -b dev
git push -u origin dev

if already exist 
git checkout master 


git clone <branch>
git checkout <branch>
git add . / git add <file1> <file2>
git status
git commit -m <commit-msg>
git push


git checkout master
git merge branch_1 / git rebase branch_1



Azure Devops CICD
###################################################################



Create Azure devops Account 

https://dev.azure.com/
organization 
project 
no access to change project name 
initialize , a readme.md file will be created 
main branch will be created 
click
create "release" branch based on main branch in azure devops 

go to databricks 

https://adb-604768739209769.9.azuredatabricks.net/
enable untiy catalog metastore01 as above , now can create catalog, database (schema), table etc as above 
Right click repos and create folder FitnessTracker, inside folder configure source control git/azure devops 
set user name password , we can username and password generate it in azure devops and feed in databricks 

In databricks create feature branches based on the release branch, and pull once always as good practice 
create notebooks in feature branch  


right click on the repository and choose git operation

Developement Lifecycle

clone the project from teh source control 
create your feature branch 
write code , unit test , commit 
raise a pull request to merge the feature branch 
then merge to master and cicd from there 

single/multinode cluster 
donot use photon 
commit code using databricks

https://learn.microsoft.com/en-us/azure/databricks/notebooks/best-practices

########databricks add ons

user settings > linked accounts > git credentials 
use developer also 
+ icon will let us add visualization 


https://adb-604768739209769.9.azuredatabricks.net/?o=604768739209769#notebook/3494381429241978/dashboard/2455937282512244
https://adb-604768739209769.9.azuredatabricks.net/?o=604768739209769#notebook/3494381429241978/dashboard/2455937282512244/present


##########################secret scope set up in databricks#########################################


Dbutils Secret Scope 
######################################################################################################################
search for azure key vault resource in azure , resource group , region etc 
access configuration vault access policy , check access policy create, list, update, create, import, delete, recover 
once resource created, under objects , secrets, generate/import , manual, create secret databricks-secrets-639
go to properties , copy vault uri and resource id

In databricks open in new tab, cut the url till # 

https://adb-604768739209769.9.azuredatabricks.net/onboarding?o=604768739209769#secrets/createScope
add the name of azure key vault , All Users,  
copy copy vault uri and resource id in databricks and click on create 
https://databricks-secrets-007.vault.azure.net/
/subscriptions/48c839e2-ee7a-40b1-8fd5-812ea7bf764c/resourceGroups/resourcegroup01/providers/Microsoft.KeyVault/vaults/databricks-secrets-007

dbutils.secrets.get("databricks-secrets-007", key="ehendpoint")
result Redacted
can 
for x in dbutils.secrets.get('databricks-secrets-007', key="ehendpoint"):
  print(x)

# can also create key versions in azure key vault




azure cli 
---------



az login
az group create --name resourcegroup01 --location eastus
az group list

az eventhubs namespace create --name testnamespaceCLI7 --resource-group resourcegroup01 -l eastus 

az eventhubs eventhub create --resource-group resourcegroup01 --namespace-name testnamespaceCLI7 --name testeventhub7 --partition-count 4 --retention-time 2 --cleanup-policy Delete

az eventhubs eventhub consumer-group  create --consumer-group-name test-consumer --eventhub-name testeventhub7 --namespace-name testnamespaceCLI7 --resource-group resourcegroup01

az eventhubs eventhub list --resource-group resourcegroup01 --namespace-name testnamespaceCLI7

az eventhubs eventhub delete --resource-group resourcegroup01 --namespace-name testnamespaceCLI7 --event-hub-name testeventhub7  

az group delete --name resourcegroup01


**************************************************************************************************************************
**************************************************************************************************************************



------------------------------------------------------------------------------------------------------------
Apache Spark 
------------------------------------------------------------------------------------------------------------


Distributed computing framework, we can process the data in multiple computers
breaks data in chunks based on split size, also keep a copy of data in other computers 
breaks actions into jobs, per wide transfromation one stage, per narrow transformation task
then 
in memmory computation, not disk to memory I/O so fast , just creates dag in mmeory lazy evaluate

hdfs, azure , s3 data is already distributed , each worker node one or many partitions of file, also copy in other nodes 
but these chunks are in disk actually 
spark lazily will load the partition in memory and call as rdd , example 4 rdd partitions, then 4 tasks, now it sees the program
narrow transformation like filter , select (narrow transformation) in one task , and execute them in parallel in computers
now these tasks are logically grouped under some stage, lets say, we need group by or join (wide transformation) 
and we need data from multiple computers memory so they are handled by stages, the also shuffle the data and talk with each other with stage buffer 
jobs trigger the construction of these execution plans by analysing the rdd lineage when an action like count, show is invoked, so dont execute so many , it will 
unnecessarily load dags in memory with data   
once all jobs are finished spark application is finished
many action so many job hence many execution plans, data loaded in memory sperately per job, until cached (this is also not guarantee), 
multiple times same data may be loaded in memory , create network traffic and resource issue will be there 
so if hitting so many action or calculation df in a loop , use cache, it is costly , also remember to uncache

The key point to understand is that data partitions are not loaded into processing cores but rather into memory. The processing cores are responsible for executing tasks on the data that is in memory. When a specific task is assigned to a core, it operates on the data that's readily available in memory, which minimizes the need to access data from disk.

This approach of loading data into memory and processing it with multiple cores in parallel is what allows Spark to achieve high performance and efficiency in distributed data processing.




Retrieving data from disk is typically more costly than accessing data from memory due to the inherent differences in their respective speeds and characteristics. Here's why retrieval from disk is slower and more costly:

Speed: Memory is significantly faster than disk. Accessing data from memory involves nanosecond-level latencies, while disk access typically has millisecond-level latencies. This speed difference can be several orders of magnitude. In computing terms, memory access is nearly instantaneous, whereas disk access involves mechanical components (in the case of hard drives) or seeks across storage media (for both hard drives and solid-state drives), leading to slower data retrieval.

Random vs. Sequential Access: Disk access often requires seeking to the appropriate location on the disk platter, which is a random access operation. In contrast, reading data from memory is generally sequential and doesn't involve physically moving components. Random access is much slower than sequential access.

Bandwidth: Memory has much higher bandwidth compared to disks. This means that data can be read from memory at a much faster rate than from disk. Disk I/O operations are often a bottleneck in data processing because they can't keep up with the processing speed of modern CPUs.

Data Transfer Rates: Disks have limited data transfer rates, while memory can deliver data at very high rates. Reading large amounts of data from disk can be time-consuming due to these limitations.

Caching: In-memory data can be cached, which means that frequently accessed data is kept readily available in memory for faster access. Caching can significantly reduce the need to fetch data from disk repeatedly.

Given these factors, modern data processing frameworks like Apache Spark aim to keep as much data as possible in memory, minimizing the need to retrieve data from disk. This in-memory processing approach greatly accelerates data operations and improves the overall performance of data-intensive tasks. Disk access is usually reserved for scenarios where data doesn't fit in memory or when data durability and persistence are required, but it's generally much slower in comparison to memory access.










1> data partitin in disk loaded to memory rdd 
2> partition in nodes task parallel , execute select , where 
3> stages make these inter node tasks to talk like group, join wide with a buffer and shuffle 
4> now job trigger these stages and tasks
5> many jobs many such exeuction units (stage and tasks), so be careful
6> same data loaded many times in memory, so hit less actions and carefully cache 


--------------------------------------------------------------------------------------------------------------------------------------
Apache spark missing features 
--------------------------------------------------------------------------------------------------------------------------------------

Data Storage Infrastructure
Acid Transaction Capabilities 
Metadata Catalog
Cluster Computing Infrastructure
Automation API tools 

---------------------------------------------------------------------------------------------------------------------------------------
Databricks 
---------------------------------------------------------------------------------------------------------------------------------------

spark on cloud 
cloud storage integration 
acid transaction via delta lake integration 
unity catalog for metadata management 
cluster computing cluster 
photon query engine 
at least 5 X faster apache spark 
notebooks and workspaces 
Code in any language and interoperate 
handle both structured and unstructured data
many ways to write code , apis for people towards sql, python and R 
administration controls 
optimized spark 
Automation tools 
SQL analytics
Admin controls 
Databases and Tables 
Data Lake Integration

-------------------------------------------------------------------------------------------------------------------------------------------
Azure Databricks 
-------------------------------------------------------------------------------------------------------------------------------------------
First part service, like any other service, Unified Billing , Not seperate billing 
azure devops 
azure active directory , single sign on , security feature 
azure storage 
azure data services (azure sql, synapse db and cosmos db )
azure monitoring and iot hub and event hub 
power bi and azure ml 
azure data factory 

portal.azure.com 

premium has azure sql and access controls than standard tier.

------------------------------------------------------------------------------------------------------------------------------------------------
Azure Databricks Architecture
------------------------------------------------------------------------------------------------------------------------------------------------

once databricks file system is created , it creates it own storage container etc, network security group, virtual network and clusters within it i.e VMs, which is used to store files from dbfs its called dataplane , in cloud, public ip , deleting cluster may delete disk etc. 

Databricks azure deployment has these parts

1> Control Plane Actual Databricks Subscription (Databricks UI, Notebooks, Jobs, Cluster Manager, DBFS data bricks file system), Databricks can be protected under a virtual network can talk to it via Rest Client, Azure CLI , can be connected via power BI
2> Data Plane Customer Subscription, created by customer (Azure Storage is created automatically by databricks in azure cloud, Azure SQL protected by Vnet, NSG (network security group), cluster also resides here, clsuter manager is also under vnet and cluster runs in data plane) , they also talk to external data sources 
3> SSO Azure AD , can access databricks without going to azure portal 

Don't sore the data in storage databricks created for itself, store in ADLS Gen 2 containers
Data can be accessed from external systems in azure cloud or outside 

creating clusters will also create disk resources 


---------------------------------------------------------------------------------------------------------------------------------------------------------
Types of Cluster
---------------------------------------------------------------------------------------------------------------------------------------------------------

single node driver executor on same machine 
multinode will create driver and executor on different machine 
minimum VM configuration is 14 GB memory and 4 CPU cores, Stand D3S are Vms 
tags are user defined meta data , to identify them in reports 
environment variables are automatically set by databricks 
can collect cluster logs 
init scripts, like shell scripts to install something before cluster starts 
DBU Databricks Units , but total cost includes dbu + azure resource cost 
1 USD as 1 DBU, its little less 

-----------------------------------------------------------------------------------------------------------------------------------------------------------
Designing Databricks LakeHouse
-----------------------------------------------------------------------------------------------------------------------------------------------------------

1> Storage Layer : Azure ADLS Gen2 Cloud Storage , compatatible with spark and delta lake , optimized for cost and performance, massive scalability,
fine grained security model, durable and highly available, hierarchical directory structure, database is automatically encrypted by ADLS gen2, proetcts 
against hardware failures.

/sbit-metastore-root (project metdata)
/sbit-managed-dev (all managed tables and delta tables) (medallion architecture /bronze_db, /silver_db, /gold_db),  copy from stage and prod
/sbit-unmanaged-dev (/data_zone, /checkpoint_zone) ,  copy from stage and prod

2> Databases Bronze, Silver, Gold will store the data in bronze , silver and gold directories respectively 

Spark application reads raw data from data zone and saves the output directly in the bronze db directory 
Transformation is applied and poutput is saved into bronze db directory 
spark application writes state information in checkpoint directory 
mount these directories in dbfs , but it doesnot have fine grained access control, access to all users in databricks 
risk of data corruption , so write to db mounted , not to container directly 
unity catalog use to add data governance 
give access to bronze , silver, gold db based on user but not the underlying files 
on top of it databricks works[ace management and cluster policies sit 
better to decouple data landing to /data_zone , can connect to event hub automatic file creation in adls gen2 container 
data_zone can again have internal subdirectories 
bronze layer may have directories per data_zone subdiretcory to hold the data 


Designing Bronze Layer : raw 
Designing Silver Layer : error filtered and schema applied and other transformations
Designing Gold Layer : curated as per business 


-----------------------------------------------------------------------------------------------------------------------------------------------------
Set up Source Control
-----------------------------------------------------------------------------------------------------------------------------------------------------

Azure Devops , GitHub
dev.azure.com	
create organziation, projects SBIT, repos , boards , intialize , create new branch called relase based on main branch 
developer multiple feature under release branch

-------------------------------------------------------------------------------------------------------------------------------------------------------
Prerequistes
-------------------------------------------------------------------------------------------------------------------------------------------------------

1> create databricks workspace 
2> create storage layer 
3> setup unity catalog and metastore (needs premium workspace, path to storage container, resource id of azure databricks access container)
4> create catalog and locations (create catalog for managed tables and define a catalog location, create a location for unmanaged tables, create a location for checkpoints)

subscription > resource group > resources 
us east cheaper region 

----------------------------------------------------------------------------------------------------------------------------------------------------------
Configure Storage Layer 
----------------------------------------------------------------------------------------------------------------------------------------------------------

1> Create Storage Account 

storage accounts > create > add subscription, resource group, region, unique name, standard, LRS/GRS , advanced enable hierarchical name space
create container sbit-metatstore-root 
create another container sbit-managed-dev 
create another container sbit-unmanaged-dev

2> Create access connector for azure databricks 

choose subsription, resource group, name , region , copy the resource id 
go back to storage account , access control iam, add, add role assignment , select storage blob data contributor role > next , select members username >
review and assign 

3> Create Unity Catalog Metatsore 

databricks workspace > username click > manage account > data tab , create metastore > name > same region where workspace and containers are created > 
paste sbit-metatsore-root@sbitsa.dfs.core.windows.net/sbit-meta , paste the connector id in the text box , create , now can choose both the dev and qa databricks workspaces 
user management >  add group > sbit-dev > add members > click on wokspace (to assign a workspace to this group) > sbit-ws workspace (dev instance) >
permissions > choose sbit-dev group > User 
do teh same thing for qa instance as well 

4> Create Catalog and Storage Locations

a> for managed bronze, silver, gold tables

In the dev databricks wokspace > catalog menu > external data > external locations > create location > give name to location > storage credential is the one 
which automatically got created when we chose teh untiy catalog > abfss://sbit-managed-dev@sbitsa.dfs.core.windows.net/

Now create catalog at this location 

click catalog > can see hive_metastore (not linked with unity catalog /user/hive/warehouse controlled by spark.sql.warehouse.dir), main, samples, system catalog i.e databases > create catalog > give catalog name > standard > sbit-managed location > create 

Note: catalog> database and schema are same thing > tables/views 

Now go to permissions > grant > sbit-dev in principles > all privileges > Grant 

b> for unmanaged tables like data_zone, checkpoints etc 

In the dev databricks wokspace > catalog menu > external data > external locations > create location > give name to location daat_zone > storage credential is the one which automatically got created when we chose the unity catalog > abfss://sbit-unmanaged-dev@sbitsa.dfs.core.windows.net/data > create 

In the dev databricks wokspace > catalog menu > external data > external locations > create location > give name to location daat_zone > storage credential is the one which automatically got created when we chose the unity catalog > abfss://sbit-unmanaged-dev@sbitsa.dfs.core.windows.net/checkpoint > create 

need to grant permissions for these external locations as well.



----------------------------------------------------------------------------------------------------------------------------------------
Start Coding
----------------------------------------------------------------------------------------------------------------------------------------


Workflow > Repos > create Folder >  create repo > fill infromation about source control > clone in repo > copy https url > generate Git cerdentials in 
azure devops > may clone part of repo using advanced

Somebody should create release branch first > create feature branch based on release branch > hit pull button 

Add Notebook to SBIt repository > Change notebook name 

---------
README.md
---------

------------
config.ipynb
------------

class Config():
  def __init__(self):
    self.base_dir_data = spark.sql("describe external location `data_zone`").select("url").collect()[0][0]
	self.base_dir_checkpoint = spark.sql("describe external location `checkpoint`").select("url").collect()[0][0]
	self.db_name = "sbit_db"
	self.maxFilesPerTrigger = 1000


-----------
setup.ipynb
-----------

# may have __ or _ at start to denote the file is private or protected 

% run ./config 


class Setuphelper():
   def __init__(self, env):
      Conf =  Config()
	  self.landing_zone = Conf.base_dir_data + "/raw"
	  self.checkpoint_base = Conf.base_dir_checkpoint + "/checkpoints"
	  self.catalog =  env 
	  self.db_name = Conf.db_name 
	  self.initialized = False 
	  
   def create_db(self):
      spark.catalog.clearCache()
	  spark.sql("Cretata database if not exists {self.catalog}.{self.db_name}")
	  spark.sql("Use catalog {self.catalog}.{self.db_name}")
	  self.initialized = True
	  print("done")
	  
	  
   def create_reg_usr(self):
	   if self.initialized:
		  print()
		  spark.sql(f"""CREATE TABLE if not exists {self.catalog}.{self.db_name}.registered_users_bz(
		  user_id long, 
		  device_id long 
		  partitioned by (topic, week_part)
		  )""")
	   else:
	     raise ReferenceError("Database is not defined. cannot create table in default database")
		 
		 
	def setup(self):
	    import time 
		start =  int(time.time())
		print("starting setup")
		self.create_db()
		self.create_reg_usr()
		print("setup completed in {int(time-time() - start)} seconds")
		
	# create a staticsmethod or classmethod for time mesasurement or may push to Utils , may create a common Utils packahe with 
	Utilities module and use it for import 
		 
	# one function per table 
	
   def assert_table(self, table_name):
     assert spark.sql("SHOW tables in {self.catalog}.{self.db_name}).filter("fisTemporary=False and tableName='{table_name}'").count()==1 , "
	 The table {self.table_name} in {self.catalog}.{self.db_name} is missing"
	 
	def validate(self):
       import time 
       start =  int(time.time())
       self.assert_table("")
       self.assert_table("")

    def cleanup(self):
       dbutils.fs.rm(self.landing_zone, True)	
	  
# To commit right click > git > give commit message > commit and push 

1> clone the project from source control 
2> create feature branch 
3> write code, unit test and commit 
4> raise a pull request to merge feature branch with release and qa team will raise merge request to merge release branch , CICD pipeline is 
started to merge the code to qa 

----------------
scratchpad.ipynb
----------------

%run /Repos/SBIT/SBIT/setup
SH =  SetupHelper("dev")
SH.cleanup()
SH.setup()
SH.validate()

--------------------
history-loader.ipynb
--------------------

%run config

class HistoryLoader():
    def __init__(self, env):
      Conf =  Config()
	  self.landing_zone = Conf.base_dir_data + "/raw"
	  self.test_data_dir = Conf.base_dir_checkpoint + "/test_data"
	  self.catalog =  env 
	  self.db_name = Conf.db_name 
    
	def load_data_lookup(self):
	    spark.sql(f"""
		INSERT overwrite into {self.catalog}.{self.db_name}.date_lookup 
		select date, week, month, dayofweek 
		from json.`{self.test_data_dir}/lookup.json/`
		""")
		
	def load_history(self):
	   import time 
	   self.load_data_lookup()
	   # calculate time 
	   
	def assert_count(self, table_name, expected_count);
	   actual_count = spark.read.table(f"{self.catalog}.{self.db_name}.{self.table_name}").count()
	   assert actual_count = expected_count , f"Expected count {self.expected_count}, {actual_count} in {table_name}"
	   
	def validate(self):
	   import time 
	   start = int(time.time())
	   self.assert_count(f"date_lookup", 365)
	   print("")
	   
-------------
bronze.ipynb 
-------------

Batch Processing Approach 
Stream Processing Approach 

1> Support batch run at regular intervals
2> Supports continous streaming run 
3> Auomatic checkpointing 
4> Automatic retstart

%run ./config


class Bronze():
   def __init__(self, env):
      Conf =  Config()
	  self.landing_zone = Conf.base_dir_data + "/raw"
	  self.checkpoint_base = Conf.base_dir_checkpoint + "/checkpoints"
	  self.catalog =  env 
	  self.db_name = Conf.db_name 
	  spark.sql("USE {self.catalog}.{self.db_name}") 
	  
   def consumer_user_registration(self):
      schema = ""
	  df_stream = spark.readStream.format("cloudFiles")
  
	  if once == True:
	     return stream_writer.trigger(availableNow=True).toTable(f"{self.catalog}.{self.db_name}.registered_users_bz")
	  else:
	     return stream_writer.trigger(processingTime=processing_time).toTable(f"{self.catalog}.{self.db_name}.registered_users_bz")
	  
	  
	def kafka_multiplex(self):
	
	
	def consume(self):
	  self.consumer_user_registration()
	  self.kafka_multiplex()
	  
	  
	def assert_count(self, table_name, expected_count);
	   actual_count = spark.read.table(f"{self.catalog}.{self.db_name}.{self.table_name}").count()
	   assert actual_count = expected_count , f"Expected count {self.expected_count}, {actual_count} in {table_name}"
	   
	def validate(self):
	   import time 
	   start = int(time.time())
	   self.assert_count(f"date_lookup", 365)
	   print("")
	  

# This catalog already is connected to managed-dev container 



------------
silver.ipynb 
------------


%run ./config


class Upserter:
  def __init__(self, merge_query, temp_view_name):
	  self.merge_query = merge_query
	  self.temp_view_name = temp_view_name 
	  
	  
  def upsert(self, df_micro_batch, batch_id):
      df_micro_batch.createOrReplaceTempView(self.temp_view_name)
	  df_micro_batch._jdf.sparkSession().sql(self.merge_query)
	  
	  
class CDCUpserter:
  def __init__(self, merge_query, temp_view_name):
	  self.merge_query = merge_query
	  self.temp_view_name = temp_view_name 
	  self.id_column = id_column
	  self.sort_by = sort_by 
	  
	  
  def upsert(self, df_micro_batch, batch_id):
      # window partition logic
      df_micro_batch.createOrReplaceTempView(self.temp_view_name)
	  df_micro_batch._jdf.sparkSession().sql(self.merge_query)


class Silver():
   def __init__(self, env):
      Conf =  Config()
	  self.checkpoint_base = Conf.base_dir_checkpoint + "/checkpoints"
	  self.catalog =  env 
	  self.db_name = Conf.db_name 
	  spark.sql("USE {self.catalog}.{self.db_name}") 
	  
   def upsert_users(self):
      # upsert query 
	  
   # one function per table 
   
   def upsert():
    # call all 8 functions 
	  
	   
   def assert_count(self, table_name, expected_count);
	   self.assert_count("users", 5 if sets==1 else 10)
	   
   def validate(self):
	   import time 
	   start = int(time.time())
	   self.assert_count(f"date_lookup", 365)
	   print("")
	   
# Read new data from bronze table 
# Remove duplicates
# Transform the raw data 
# Merge the transformed data into the silver table 


data_upserter = Upserter(query, "users_delta")
df_delta.writeStream.foreachBatch(data_upserter.upsert).outputMode("update").option("checkpointLocation", f"{self.checkpoint_base}/users")
.queryName("users_upsert_stream")


# for multiple records updating one record, rank based on time_stamp, and apply the latest update 

----------
gold.ipynb
----------

%run ./config


class Upserter:
  def __init__(self, merge_query, temp_view_name):
	  self.merge_query = merge_query
	  self.temp_view_name = temp_view_name 
	  
	  
  def upsert(self, df_micro_batch, batch_id):
      df_micro_batch.createOrReplaceTempView(self.temp_view_name)
	  df_micro_batch._jdf.sparkSession().sql(self.merge_query)
	  
	  
class CDCUpserter:
  def __init__(self, merge_query, temp_view_name):
	  self.merge_query = merge_query
	  self.temp_view_name = temp_view_name 
	  self.id_column = id_column
	  self.sort_by = sort_by 
	  
	  
  def upsert(self, df_micro_batch, batch_id):
      # window partition logic
      df_micro_batch.createOrReplaceTempView(self.temp_view_name)
	  df_micro_batch._jdf.sparkSession().sql(self.merge_query)


class Gold():
   def __init__(self, env):
      Conf =  Config()
	  self.test_data_dir = Conf.base_dir_checkpoint + "/test_data "
	  self.checkpoint_base = Conf.base_dir_checkpoint + "/checkpoints"
	  self.catalog =  env 
	  self.db_name = Conf.db_name 
	  spark.sql("USE {self.catalog}.{self.db_name}") 
	  
   def upsert_users(self):
      # upsert query 
	  
   # one function per table 
   
   def upsert():
    # call all 8 functions 
	  
	   
   def assert_count(self, table_name, expected_count);
	   self.assert_count("users", 5 if sets==1 else 10)
	   
   def assert_rows(self, location, table_name, sets):
	  expected_rows = spark.read.format("parquet").load("{self.test_data_dir}/location.parquet").collect()
	  actual_rows = spark.table(table_name).collect()
	  assert expected_rows = actual_rows, "Expected data mismatches with actual data in {table_name	}"
	   
   def validate(self):
	   import time 
	   start = int(time.time())
	   self.assert_count(f"date_lookup", 365)
	   print("")
	   
------------	   
run.ipynb
------------

# environment 
# run type 
# processing time 

dbutils.wdigets.text("Environment", "dev", "set the current environment/catalog/name")
dbutils.wdigets.text("RunType", "once", "Set once to run as a batch")
dbutils.wdigets.text("ProcessingTime", "5 seconds", "Set the microbatch interval")

env = dbutils.widgets.get("Environment")
once = dbutils.widgets.get("RunType")=="once" else False
processing_time = dbutils.widgets.get("ProcessingTime")
if once:
 print("staring in batch mode")
else:
 print("starting in stream mode {processing time} microbatch.")
 
 
spark.conf.set("spark.sql.shuffle.partitions", sc.defaultParallelism) # = number of executor cores 
spark.conf.set("spark.databricks.delta.optimizewrite.enabled", True) 
spark.conf.set("spark.databricks.delta.autocompact.enabled", True)
spark.conf.set("spark.sql.streaming.stateStore.providerClass", "com.databricks.sql.streaming.state.
RocksDBStateStoreProvider")


%run ./setup 

%run ./history 

SH = SetupHelper(env)
HL = HistoryLoader(env)



setup_required = spark.sql(f"SHOW DATABASES IN (SH.catalog}").filter(f"databaseName == '{SH.db_name}").count() != 1 
if setup_required:
	SH.setup()
	SH.validate()
    HL.load_history()
	HL.validate()
else:
	spark.sql(f"USE (SH.catalog). {SH.db_name}")]
	
	
%run bronze 

%run silver 

%run gold 

BZ = Bronze(env)
SL = Silver(env)
GL = Gold(env)

BZ.consume(once, processing_time)
SL.upsert(once, processing_time)
GL.upsert(once, processing_time)


# create workflow, divide task as necessary 

---------------------
Integration Testing
---------------------

create test data and expected results 

First then incremental 

Producer 

class Producer():
  def __init__(self):
    self.config = Config()
	self.landing_zone = self.Conf.base_dir_data + "/raw"
	self.test_data_dir = self.Conf.base_dir_data + "/test_data"
	
  def user_registration(self, set_num):
    source = 
	target = 
	dbutils.fs.cp(source, target)
	
  def profile_cdc(self, set_num):
    source = 
	target = 
	dbutils.fs.cp(source, target)

  def produce(self, set_num);
    # call all producers 
	
  def _validate_count():
  
  def validate():
    self._validate_count()
	
	
-------------------------
batch-test
-------------------------


dbutils.widgets.text("Environment", "dev", "Set the current environment/catalog name")
env = dbutils.widgets.get("Environment")


%run ./setup


SH = SetupHelper (env)
SH.cleanup


dbutils.notebook.run("./run", 688, {"Environment": env, "RunType": "once"})


%run ./history-loader


HL = HistoryLoader (env)
SH.validate()
HL.validate


%%run ./producer
PR =Producer() 
PR.produce(1)
PR.validate(1)
dbutils.notebook.run("./97-run", 688, {"Environment": env, "RunType": "once"})

%run bronze 
%run silver 
%run gold 


BZ = Bronze (env) 
SL = Silver (env) 
GL = Gold (env) 
BZ.validate(1)
SL.validate(1)
GL.validate(1)



PR.produce(2)
PR.validate(2)
dbutils.notebook.run("./87-run", 600, {"Environment": env, "RunType": "once"})
BZ.validate(2)
SL.validate(2)
GL.validate(2)
SH.cleanup()


-----------
stream-test 
-----------

Stream mode testing
Initial Setup

1. Cleanup
2. Create and trigger a Job to execute Run Notebook 
3. Validate History load

Batch 1

4. Produce Payload 1 to the landing zone 
5. Validate output for Payload 1

Batch 2

6. Produce Payload 2 to the landing zone 
7. Validate output for Payload 2




dbutils.widgets.text("Environment", "dev", "Set the current Environment/catalog name") 
dbutils.widgets.text("Host", "Databricks Workspace URL")
dbutils.widgets.text("AccessToken", "", "Secure Access Token")


env = dbutils.widgets.get("Environment")
host = dbutils.widgets.get("Host")
token = dbutils.widgets.get("AccessToken")


%run ./setup


SH = SetupHelper(env)
SH.cleanup()

# Cluster JSON configuration

# Rest API Creating a streaming job


https://docs.databricks.com/api/azure/workspace/introduction

-------------
CICD Pipeline 
-------------

CI :- Build 
CD :- Release

Project Development and CI/CD Pipeline:

Project development completed
Implementation of automated CI/CD (Continuous Integration/Continuous Deployment) pipeline
Collaboration with DevOps experts for CI/CD implementation
Effort to understand essential aspects of CI/CD
Initial overview of the CI/CD pipeline and its components
CI/CD Pipeline Overview:

CI (Continuous Integration) and CD (Continuous Deployment)

Distinction between build and release pipelines
Use of Azure DevOps for pipeline implementation
Connecting Databricks with code repositories (e.g., Azure Git Repos)
Flexibility in selecting different Git repositories (Git Enterprise, Bitbucket, AWS CodeCommit)


Code Repository and Branches:

The presence of main, release, dev, and feature branches in the code repository
Committing code to feature branches
Automatic notification to the Azure DevOps build pipeline
Initiation of the build pipeline, which involves the use of an agent (Ubuntu virtual machine)


Build Pipeline Steps:

Building agent machine
Execution of build pipeline code
Actions performed by the pipeline, such as installing Python, PyTest, and Databricks Connector
Main goal: Checking out the latest code from the Git repository
Connecting to Databricks environment and executing unit tests
Running unit tests locally on the agent or remotely using Databricks Connector
Preparing deployable artifacts, including notebooks and Python wheels
The significance of the build pipeline: Integration, testing, and artifact preparation

CI Pipeline Summary:

Build pipeline runs on all branches (feature, dev, release, main)
Objectives: Integrate code, run unit tests, reject or accept code based on test results, prepare artifacts

Release Pipeline (CD):

Distinct from the build pipeline
Limited to running on release and main branches
Triggered manually, with the option for automation
Workflow: Merging feature branches into the dev branch, creating a pull request, and notifying the build pipeline
Manual triggering for control and approval process
Execution of the release pipeline: Creating an agent machine, extracting the latest artifact, and deploying it to the target environment
Deployment tasks: Copying notebooks, installing Python wheels, and handling dependencies
Optional integration testing in the release pipeline


Overall Understanding:

Insight into the CI/CD approach for Databricks and Spark Projects
Upcoming topic: Creating both pipelines step by step for a detailed understanding of the process.


Build Pipeline 

1> Login to Azure Devops , Go to Project , Go to repos 
2> Create new feature branch based on release branch 
3> create a new file azure_build_pipeline.yaml # copy yaml file 
4> merge this file to the release branch 
5> click on pipelines, create pipeline, choose azure repos, select project, choose existing azure pipeline yaml file # or under configure > starter pipeline > add publish pipeline artificat from right search > give artifact a name notebook > azure pipelines as artifcat publish location > add > task gets added to bottom 
6> select release branch , select yaml file in path 
7> click on continue 
8> click down arrow button , save 
9> when we commit to release branch or create a pull request to release branch , It will automatically run 

Release Pipeline 

1> Login to Azure Devops , Go to Project
2> Pipelines > Releases 
3> Hit New Pipeline 
4> Click on Empty Job > + Add in Artifacts 
5> Choose Project Name, SBIT in build pipeline > Add
6> Stage 1 > click on 0 task > Agent Job > Agent Pool as Azure Pipelines > agent specifcation as ubuntu-22.04 > Save (versions depend on what databricks is using )
7> Left Side Agent Job, click + button to add tasks , search use python version , use python 3.X > change display name to Use Python 3.10 and version spec to 3.10
8> Hit Save Button at top 
9> click + button again in agent job , search extract file task and add it . click on extract files task 
10> archive files pattern **/*.zip , destination folder ${Release.PrimaryArtifactSourceAlias}/Databricks 
11> check clean destination folder before extracting , hit save button 
12> click + button again and search for bash , add bash task , click on bash script 
13> change display name to Install Databricks CLI , choose Inline Radio button , script pip install databricks-cli  > Save 
14> click on + button again to add a bash task and click on bash script to modify the properties 
15> change display name to deploy notebook , choose inline radio button , in script write 

databricks workspace import_dir --overwrite
$(System.ArtifactsDirectory)/$(Release.PrimaryArtifactSourceAlias)/Databricks//Shared/

Hit Save button 
16> Click + button once again , search for bash and add , click bash task, change display name to Run Integration test for Batch Job 
17> Inline > paste bash script to run the integration test and click on save 
18> change the name of the pipeline SBIT Deploy Pipeline 
19> Go to sbit-qa databricks workspace , copy the workspace url till .net 
20> click on email on top right and user settings menu > developer > access tokens > generate new token , for devops pipeline as comment 
21> click workspace menu in databricks , check the shared folder 
22> back to azure portal, click variables > add > DATABRICKS_HOST paste workspace url, DATABRICKS_TOKEN past access token , hit save button 
23> Release pipeline is manual , It will deploy project to QA environment 
24> Go to repos , choose the feature branch where we are devloping project > pull requests > new pull requests 
<feature branch> into <release branch> at top , Give title as initial release , hit create button , complete to complete the merge 
25> Go to pipeline and click on pipelines , a build would have run automatically with green tick , click on it and we can see the pr details 
26> click again and we can see the job status , click job and can see list of tasks , click the task to see the logs 
27> click the releases , create release button hit , create button 
28> can see stage1 running > go to deployments tab > can see stage 1 running > click on stage1 to see the running jobs 
29> can see jobs marked with green as they are completed 
30> can see all codes deployed to qa databricks workspace , in workflow tab , can see 1 test job , go to job runs and can see test job status 

-------------------------------------
yaml file (azure_build_pipeline.yaml)
-------------------------------------

trigger:
  batch: true
  branches:
    include:
      - '*'

pool:
  vmImage: ubuntu-22.04

steps:
  - task: UsePythonVersion@0
    displayName: 'Use Python 3.10'
    inputs:
      versionSpec: 3.10

  - script: |
      pip install pytest requests setuptools wheel 
    displayName: 'Load Python dependencies'

  - checkout: self
    persistCredentials: true
    clean: true

  - script: | 
      cp $(Build.Repository.LocalPath)/*.* $(Build.BinariesDirectory)/ 
    displayName: 'Get Changes'

  - task: ArchiveFiles@2
    inputs:
      rootFolderOrFile: '$(Build.BinariesDirectory)'
      includeRootFolder: false
      archiveType: 'zip'
      archiveFile: $(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip"
      replaceExistingArchive: true

  - task: PublishBuildArtifacts@1
    inputs:
      ArtifactName: 'DatabricksBuild'


------------
requirements
-------------

%pip install -r /dbfs/requirements.txt

or 

dbutils.library.installPyPI("torch")
dbutils.library.installPyPI("scikit-learn", version="1.19.1")
dbutils.library.installPyPI("azureml-sdk", extras="databricks")
dbutils.library.restartPython()  # Removes Python state, but some libraries might not work without calling this function

%run /path/to/notebook_install_lib







######sql , display if id in either (can use OR or sperate using union all)

SELECT
    COALESCE(t1.id, t2.id, t3.id) AS id,
    t1.column1 AS t1_column1, t1.column2 AS t1_column2,
    t2.column1 AS t2_column1, t2.column2 AS t2_column2,
    t3.column1 AS t3_column1, t3.column2 AS t3_column2
FROM
    table1 t1
FULL OUTER JOIN
    table2 t2 ON t1.id = t2.id
FULL OUTER JOIN
    table3 t3 ON COALESCE(t1.id, t2.id) = t3.id;

#############which columsn are not null without case when

SELECT 
    id, 
    TRIM(
        COALESCE(col1 || ' is not null. ', '') || 
        COALESCE(col2 || ' is not null. ', '') || 
        COALESCE(col3 || ' is not null. ', '')
    ) AS not_null_columns
FROM 
    your_table;


select t1.id, t1.name, case when (t1.class = t2.class and t1.age = t2.age) then True else False end 
from tbl1 t1
--left 
join tbl2 t2 on coalesce(trim(t1.id), 'id') = coalesce(trim(t2.id), 'id') and coalesce(trim(t1.name), 'name') = coalesce(trim(t2.name), 'name');


--left join as inner join 

select t1.id, t1.name
from tbl1 t1
left join tbl2 t2 on coalesce(trim(t1.id), 'id') = coalesce(trim(t2.id), 'id') and coalesce(trim(t1.name), 'name') = coalesce(trim(t2.name), 'name') where t2.id is not null and t2.name is not null;


SELECT t2.id, t2.address
WITH m1 AS (
SELECT t1.id AS t1_id, t1.address AS t1_address, t2.id AS t2_id
, t2.address AS t2_address
FROM history as t1 FULL OUTER JOIN new AS t2
WHERE t1.id=t2.id
)
SELECT t2_id, t2_address FROM m1 WHERE t2_id IS not null
union all
SELECT t1_id,  t1_address FROM m1 WHERE t2_id IS null

